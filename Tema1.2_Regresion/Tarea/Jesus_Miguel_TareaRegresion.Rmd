---
title: "Tarea -- Regresión lineal"
author: "Jesús Martínez Leal y Miguel Muñoz Blat (Grupo 11)"
date: "`r Sys.Date()`"
output:
  html_document:
    echo: yes
    number_sections: no
    theme: readable
    toc: yes
    fig_caption: yes
  pdf_document:
    toc: yes
    fig_caption: yes
  html_notebook: 
    echo: yes
    number_sections: no
    theme: readable
    toc: yes
    fig_caption: yes
params:
  lang: ES
lang: "`r switch(params$lang, ES = 'es-ES', EN = 'en-US')`"
subtitle: "Tema 1 - Aprendizaje Máquina (I). Master Ciencia de Datos UV"
header-includes:
   - \usepackage{multirow}
   - \usepackage{tikz} # Diagramas Latex
   - \usetikzlibrary{positioning}
   - \usepackage{float}
   - \usepackage[font=small,labelfont=bf,labelsep=period]{caption}
   - \usepackage{subcaption}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# Configuración general de chunks
library(knitr)
options(width = 100)
knitr::opts_chunk$set(echo = F, message = T, error = T, warning = F, comment = NA, dpi = 100, tidy = F, cache.path = '.cache/',  fig.path = './figure/', include = T, fig.cap = "", fig.width = 6, fig.height = 4.5)
```

```{r librerias, message = F, include = T, echo = F}
# Carga de librerías necesarias con pacman
library(pacman)
pacman::p_load(readr, ggplot2, dplyr, tidyr, MASS, leaps)
```

# Ejercicio 1.

Considera la variable respuesta `Price` relacionándola con la variable `X` con la que tenga mayor relación lineal.

1. Evalúa el efecto de `X` sobre `Price`.

2. Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad,
R2, contraste del modelo, etc...).

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de confianza al 90 %.

4. Realiza un diagnóstico de los residuos. Si falla algunas de las condiciones, busca una posible
solución.


## Carga inicial de los datos necesarios.

```{r carga_cars}
cars <- read.csv2("./data/cars.csv", stringsAsFactors = TRUE)
```

Lo primero de todo será ver un poco en qué consiste nuestro conjunto de datos y en qué unidades se miden las distintas variables.

```{r info_cars}
head(cars)
```
Tras usar el comando `?Cars93` en la terminal de RStudio, observamos que tenemos numerosas variables con características de 93 coches en venta en Estados Unidos en 1993. Vemos que por ejemplo la variable `Price` viene medida en miles de dólares y que `Horsepower` viene medida en HP (lo que serían 746 W en el Sistema Internacional).

## Apartado 1.

Evalúa el efecto de `X` sobre `Price`.

**Resolución**.

Para evaluar el efecto de una variable `X` de nuestro conjunto sobre `Price` haremos una correlación entre los pares de variables. Se utiliza el método de Pearson para su cálculo.

La expresión de la correlación viene dada por:

\begin{equation}
r_{X, Y} = \frac{s_{x,y}} {s_x s_y},
\end{equation}

siendo $s_{x, y}$ la covarianza entre las variables $X$ e $Y$ y $s_{x}, s_{y}$ las desviaciones estándar de las variables $X$ e $Y$, respectivamente.

```{r cor_price}
cors_Price <- cars %>%
  summarise(across(where(is.numeric), ~ cor(Price, ., method = "pearson")))
cors_Price

# Miro cuál es la variable cuyo valor absoluto en correlación es mayor (excluyendo Price)

max_abs_value <- max(abs(cors_Price[!(names(cors_Price) == "Price")]))
var_max_value <- names(cors_Price)[which(abs(cors_Price) == max_abs_value)]

max_value <- cors_Price[[var_max_value]]
```

Como se puede apreciar, la mayor correlación en valor absoluto más cercana a la unidad viene dada por la variable `var_max_value` con un valor de `r max_value`.

## Apartado 2.

Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, R2, contraste del modelo, etc...).

**Resolución**.

Nuestro modelo de regresión lineal se considera de la forma:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon,
\end{equation}

siendo en este caso $Y$ = $Price$ y $X$ = $Horsepower$.

```{r lm_horsepower_price}
reg <- lm(Price ~ Horsepower, data = cars)
sum_reg <- summary(reg) # Resumen del modelo de regresión lineal
sum_reg
confint(reg, level = 0.90)
```
En los coeficientes podemos apreciar que el valor estimado del `intercept` ($\beta_0$) es `r reg$coefficients[1]` y el de la pendiente ($\beta_1$) es `r reg$coefficients[2]`. A pesar de que el intercepto posee un valor negativo (un precio negativo carecería de sentido) se nos indica que no hay significancia, por lo que no se descarta que fuera nulo. En contra, la pendiente sí posee significancia, indicando que efectivamente existe una relación (lineal) entre `Price` y `Horsepower` y que además es positiva. Esto es lo que cabría esperar: una subida del precio del vehículo conforme aumentan los caballos de potencia. Se muestran también los intervalos de confianza al 90% para intercepto y pendiente. Esto permite reafirmar lo comentado anteriormente: el intercepto no se descarta que sea 0, mientras que la pendiente sí es claramente distinta de 0 y además positiva.$\\$

Por otra parte, el valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`. El valor ajustado del $R^2$ viene dado por la expresión:

\begin{equation}
R^2_{\textrm{ajustado}} = 1 - \frac{n - 1}{n - k - 1} (1 - R^2)
\end{equation}

Este tiene en cuenta la penalización incluida por aumentar el número de predictores. El valor obtenido es ligeramente menor: `r sum_reg$adj.r.squared`. $\\$

Finalmente, el p-valor asociado al estadístico F es también sufucientemente pequeño (< 2.2e-16) como para concluir que al menos un coeficiente del modelo lineal es distinto de cero. En este caso, dado que solo hay un predictor, este test sería equivalente al contraste sobre la pendiente de la recta (por eso se obtiene lo mismo).

## Apartado 3.

Dibuja el diagrama de dispersión, la recta de regresión y las bandas de confianza al 90 %.

**Resolución**.

```{r graph_hp_price, fig.cap = "\\label{fig:graph_hp_price}Gráfico de dispersión y recta de regresión para nuestro modelo", fig.pos = "H", fig.align = "center"}

# Gráfico de dispersión con rango en el eje x de 0 a 300
with(cars, plot(Horsepower, Price, col = 'blue', xlab = 'Horsepower / HP', ylab = 'Price / 1000 $', xlim = c(0, 300), ylim = c(-10, 70)))

# Recta de regresión
abline(reg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_hp <- data.frame(Horsepower = seq(0, 300, length.out = 250))
bandas <- predict(reg, newdata = puntos_hp, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_hp$Horsepower, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_hp$Horsepower, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topleft", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
Observamos en la Figura anterior cómo la mayoría de puntos con los que contamos están agrupados en torno a 100 - 200 de Horsepower medido en HP. El modelo lineal realmente solo tiene relevancia en la zona de nuestros datos. Puede observarse cómo las bandas de confianza se acercan mucho más a la recta de regresión donde teníamos más valores agrupados. Es notable apreciar cómo hay un valor muy alejado de nuestra recta de regresión que seguramente está "tirando de ella" hacia arriba.

## Apartado 4.

Realiza un diagnóstico de los residuos. Si falla algunas de las condiciones, busca una posible solución.

**Resolución**

Realizamos primero un diagrama de dispersión para nuestros residuos.

```{r graph_res, fig.cap = "\\label{fig:graph_res}Gráfico de residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
plot(reg$fitted.values, reg$residuals, col='blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```

En el gráfico de residuos frente a valores predichos podemos ver que la variabilidad no es estrictamente constante. Para precios bajos encontramos una menor variabilidad que la hallada para precios intermedios y altos. Esto nos lleva a pensar que no tenemos una homocedasticidad clara. Para tratar de solucionar esto, podemos probar a realizar transformaciones sobre la variable dependiente (Precios).
Por otra parte, la linealidad sí que parece adecuada: los residuos se distribuyen alrededor del 0.

En cuanto a la normalidad de los residuos encontramos esto:

```{r qqres, fig.cap = "\\label{fig:qq_res}QQ-Plot de los residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
qqnorm(reg$residuals, col='blue', main = 'QQ-Plot de los Residuos')
qqline(reg$residuals)

shap <- shapiro.test(reg$residuals)
shap
```

Vemos cómo la distribución de residuos sigue un comportamiento normal pero difiere en gran medida en las colas. Utilizando el test de normalidad de Shapiro-Wilk obtenemos un p-valor de `r shap$p.value`, por lo que se rechaza la hipótesis nula de que la distribución sea normal para un nivel de significancia de 0.05.

Veremos ahora cómo cambian las cosas en nuestro modelo si aplicamos una transformación logarítmica.

### Transformación logarítmica de la variable `Precio`

Realizamos los mismos pasos anteriores pero modificando `Precio` por `Log(Precio)` como variable dependiente.


```{r lm_horsepower_logprice}
logreg <- lm(log(Price) ~ Horsepower, data = cars)
sum_logreg <- summary(logreg) # Resumen del modelo de regresión lineal
sum_logreg
confint(logreg, level = 0.90)
```

Con el modelo logarítmico apreciamos que el valor de $R^2$ ha subido algo, teniendo `r sum_logreg$r.squared`, lo que indicaría en este caso una ligera mejoría de lo que teníamos anteriormente. Esto se debe principalmente a que el "outlier" que se ha comentado anteriormente ha sido reducido en importancia con esta transformación.

```{r graph_hp_logprice, fig.cap = "\\label{fig:graph_hp_logrice}Gráfico de dispersión y recta de regresión para nuestro modelo tras la transformación logarítmica", fig.pos = "H", fig.align = "center"}

# Gráfico de dispersión con rango en el eje x de 0 a 300
with(cars, plot(Horsepower, log(Price), col = 'blue', xlab = 'Horsepower / HP', ylab = 'Log(Price / 1000 $)', xlim = c(0, 300)))

# Recta de regresión
abline(logreg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_hp <- data.frame(Horsepower = seq(0, 300, length.out = 250))
bandas <- predict(logreg, newdata = puntos_hp, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_hp$Horsepower, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_hp$Horsepower, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topleft", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```

```{r graph_logres, fig.cap = "\\label{fig:graph_logres}Gráfico de residuos en nuestro modelo tras la transformación logarítmica", fig.pos = "H", fig.align = "center"}
plot(logreg$fitted.values, logreg$residuals, col = 'blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos en modelo logarítmico")
abline(h = 0,lty = 2)
```

Puede observarse cómo con el modelo logarítmico se ha podido disminuir ligeramente la homocedasticidad, en el sentido de que no están tan agrupados los puntos iniciales entre sí. 


```{r qqlogres, fig.cap = "\\label{fig:qq_logres}QQ-Plot de los residuos en nuestro modelo con transformación logarítmica", fig.pos = "H", fig.align = "center"}
qqnorm(logreg$residuals, col='blue', main = 'QQ-Plot de los Residuos en transformación logarítmica')
qqline(logreg$residuals)

logshap <- shapiro.test(logreg$residuals)
logshap
```
En este caso obtenemos que nuestra distribución de residuos se adapta más a una distribución normal, viéndose además reflejado esto en el shapiro test donde se obtiene un p-valor de `r logshap` > 0.05. Así pues, no puede rechazarse la hipótesis nula de normalidad.

# Ejercicio 2.

Considera la variable respuesta Price relacionandola con el predictor MPG.city.

1. Evalúa el efecto de MPG.city sobre Price.

2. Obtén la recta mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad,
R2, contraste del modelo, etc...).

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de predicción al 90 %.

4. Realiza un análisis de los residuos.

5. ¿Te parece adecuado haber realizado regresión lineal o es preferible otro tipo de regresión?. Ajusta
el modelo que te parezca más adecuado.

6. ¿Qué precio mínimo se espera para aquellos coches con un consumo de 12 litros a los 100 km por
ciudad? Calcula e interpreta el intervalo de confianza y el de predicción.

## Apartado 1.

Evalúa el efecto de MPG.city sobre Price.

**Resolución**.

En primer lugar, de ojear en `?Cars93` vemos que `MPG.city` es básicamente `Millas por galón estadounidense de ciudad`. Es una de las métricas usadas para evaluar la economía del combustible de un vehículo. Una vez sabido esto ya tenemos un poco más de contexto acerca de lo que estamos haciendo. $\\$

La correlación entre `Price` y `MPG.city` se muestra a continuación.

```{r cor_mpg_price}
cor_Price_MPG <- cor(cars$Price, cars$MPG.city, method = "pearson")
cor_Price_MPG
```
MPG.city y Price presentan una correlación lineal de `r cor_Price_MPG`. Su valor absoluto está algo alejado de la unidad, por lo que la relación no es del todo lineal. Por otra parte, el signo negativo indica que, en general, un aumento de MPG.city conllevaría una disminución del Precio.

## Apartado 2.

Obtén la recta mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, R2, contraste del modelo, etc...).

**Resolución**.

Nuestro modelo de regresión lineal se considera de la forma:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon,
\end{equation}

siendo en este caso $Y$ = $Price$ y $X$ = $MPG.city$.

```{r lm_MPGcity_Price}
reg <- lm(Price ~ MPG.city, data = cars)
sum_reg <- summary(reg) # Resumen del modelo de regresión lineal
sum_reg
confint(reg, level = 0.90)
```
En los coeficientes podemos apreciar que el valor estimado del `intercept` ($\beta_0$) es `r reg$coefficients[1]` y el de la pendiente ($\beta_1$) es `r reg$coefficients[2]`. El intercepto posee un valor claramente distinto de 0 y además cuenta con significancia. El valor de la pendiente por su parte es negativo y cuenta con significancia. Se muestran también los intervalos de confianza al 90% para intercepto y pendiente. Esto permite reafirmar lo comentado anteriormente: el intercepto no se descarta que sea 0, mientras que la pendiente sí es claramente distinta de 0 y además negativa.$\\$

Por otra parte, el valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`. Este valor es ciertamente bajo y está algo lejos de lo que sería lo usualmente aceptable.

El valor ajustado del $R^2$ viene dado por la expresión:

\begin{equation}
R^2_{\textrm{ajustado}} = 1 - \frac{n - 1}{n - k - 1} (1 - R^2)
\end{equation}

Este tiene en cuenta la penalización incluida por aumentar el número de predictores. El valor obtenido es ligeramente menor: `r sum_reg$adj.r.squared`. $\\$

Finalmente, el p-valor asociado al estadístico F es también sufucientemente pequeño (< 3.31e-10) como para concluir que al menos un coeficiente del modelo lineal es distinto de cero. En este caso, dado que solo hay un predictor, este test sería equivalente al contraste sobre la pendiente de la recta (por eso se obtiene lo mismo).

## Apartado 3.

Dibuja el diagrama de dispersión, la recta de regresión y las bandas de predicción al 90 %.

**Resolución**.

```{r graph_mpg_price, fig.cap = "\\label{fig:graph_mpg_price}Gráfico de dispersión y recta de regresión para nuestro modelo", fig.pos = "H", fig.align = "center"}

with(cars, plot(MPG.city, Price, col = 'blue', xlab = 'MPG.city / MPG', ylab = 'Price / 1000 $'))

# Recta de regresión
abline(reg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_mpg <- data.frame(MPG.city = seq(0, max(cars$MPG.city), length.out = 250))
bandas <- predict(reg, newdata = puntos_mpg, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_mpg$MPG.city, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_mpg$MPG.city, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topright", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
Se hace muy evidente con la representación que la variable `MPG.city` tiene un aspecto algo cuantizado, ya que no posee demasiados valores distintos. Esto hace que para un mismo valor de esta, haya muchos valores de `Price` diferentes.

## Apartado 4.

Realiza un análisis de los residuos.

**Resolución**.

Realizamos primero un diagrama de dispersión para nuestros residuos.

```{r graph_mpg_res, fig.cap = "\\label{fig:graph_mpg_res}Gráfico de residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
plot(reg$fitted.values, reg$residuals, col='blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```
En la gráfica de residuos frente a valores predichos podemos ver que la relación entre ambas no es del todo lineal, tal y como podía verse tanto en la gráfica de dispersión anterior como en el valor absoluto de la correlación de Pearson. Vemos además que la variablidad no es constante por lo que también falla la homocedasticidad. 
En cuanto a la normalidad de los residuos encontramos esto:

```{r qqres_mpg, fig.cap = "\\label{fig:qq_mpg_res}QQ-Plot de los residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
qqnorm(reg$residuals, col='blue', main = 'QQ-Plot de los Residuos')
qqline(reg$residuals)

shap <- shapiro.test(reg$residuals)
shap
```

Por otro lado, en la gráfica de los cuantiles de los residuos frente a los cuantiles teóricos de una distribución normal podemos ver cómo en los extremos los cuantiles se desvían considerablemente, por lo que tampoco parece que se cumpla la normalidad de los residuos. Esto puede verse también con un contraste de hipótesis que nos brinda el test de normalidad de Shapiro-Wilk, en el que se obtiene un p-valor de `r shap$p.value`, por lo que se rechaza la hipótesis nula de que la distribución sea normal para un nivel de significancia de 0.05. $\\$


En definitiva, no se cumplen las hipótesis de partida del modelo lineal, por lo que no sería lo más apropiado para describir la relación entre ambas variables. En todo caso, podría plantearse la transformación de alguna de las variables con tal de obtener mayor linealidad, homocedasticidad y normalidad. También podría plantearse otro modelo de regresión no lineal como el **modelo potencial**, que parece más adecuado para describir la relación entre estas varaibles.

## Apartado 5.

¿Te parece adecuado haber realizado regresión lineal o es preferible otro tipo de regresión? Ajusta el modelo que te parezca más adecuado.

**Resolución**.

Se probarán distintos modelos para ver con cuál se obtienen resultados más óptimos.

### Paso a un modelo potencial.

El modelo potencial que proponemos es de la forma: 

\begin{equation}
Y = \beta_0 X^{\beta_{1}}
\end{equation}

Este puede expresarse como un modelo lineal tomando logaritmos:

\begin{equation}
\log(Y) = \log(\beta_0) + \beta_1 \log(X)
\end{equation}

Pasan a realizarse los pasos seguidos anteriormente aplicando esta última transformación.

```{r cor_logmpg_logprice}
cars$log_MPG.city <- log(cars$MPG.city)

cor_logPrice_logMPG <- cor(log(cars$Price), cars$log_MPG.city, method = "pearson")
cor_logPrice_logMPG
```
Vemos ya que la correlación ha aumentado (en valor absoluto) entre estas dos variables, lo que es una buena señal.

```{r lm_logMPGcity_logPrice}
logreg <- lm(log(Price) ~ log_MPG.city, data = cars)
sum_logreg <- summary(logreg) # Resumen del modelo de regresión lineal
sum_logreg
confint(logreg, level = 0.90)
```
Observamos ya una subida en el valor de $R^2$:`r sum_logreg$r.squared`, indicando que este modelo puede ajustarse de una mejor manera a nuestros datos.

```{r graph_logmpg_logprice, fig.cap = "\\label{fig:graph_logmpg_logprice}Gráfico de dispersión y recta de regresión para nuestro modelo potencial", fig.pos = "H", fig.align = "center"}

# Crear el gráfico de dispersión
with(cars, plot(log_MPG.city, log(Price), col = 'blue', xlab = 'log(MPG.city / mpg)', ylab = 'log(Price / 1000 $)'))

# Añadir la línea de regresión
abline(coef = coefficients(logreg), col = 'red')

# Generar puntos para el eje x
puntos_log_mpg.city <- data.frame(log_MPG.city = seq(min(cars$log_MPG.city), max(cars$log_MPG.city), length.out = 200))

# Predecir valores y bandas de confianza
bandas <- predict(logreg, newdata = puntos_log_mpg.city, interval = "confidence")

# Graficar las bandas de confianza
lines(puntos_log_mpg.city$log_MPG.city, bandas[, 2], col = 'black') # Límite inferior
lines(puntos_log_mpg.city$log_MPG.city, bandas[, 3], col = 'black') # Límite superior


# Leyenda de regresión y bandas
legend("topright", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
```{r graph_logmpg_res, fig.cap = "\\label{fig:graph_mpg_res}Gráfico de residuos en nuestro nuevo modelo potencial", fig.pos = "H", fig.align = "center"}
plot(logreg$fitted.values, logreg$residuals, col='blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```
Vemos cómo el problema de la heterocedasticidad se ha ido en gran medida, al igual que el de la linealidad. $\\$

En cuanto a la normalidad de los residuos podemos aplicar el contraste que nos ofrece el shapiro test nuevamente, obteniendo:

```{r qqlogresmpg, fig.cap = "\\label{fig:qq_logresmpg}QQ-Plot de los residuos en nuestro modelo con transformación potencial", fig.pos = "H", fig.align = "center"}
qqnorm(logreg$residuals, col='blue', main = 'QQ-Plot de los Residuos en transformación logarítmica')
qqline(logreg$residuals)

logshap <- shapiro.test(logreg$residuals)
logshap
```
Ahora sí vemos que los residuos parecen mucho más normales, obteniendo un p-valor de `r logshap$p.value`: no se puede rechazar la hipótesis nula de normalidad.

## Apartado 6.

¿Qué precio mínimo se espera para aquellos coches con un consumo de 12 litros a los 100 km por ciudad? Calcula e interpreta el intervalo de confianza y el de predicción.

**Resolución**

Para hacer este cálculo debemos primero conocer la conversión a las unidades que estamos tratando con nuestros datos. Una búsqueda en [Convertidor](https://www.advancedconverter.com/es/otros-convertidores/consumo-de-combustible/millas-por-galon-us-a-kilómetros-por-litro) nos muestra que:

\begin{equation}
1 \ \textrm{km / L} = 2.3521 \ \textrm{mpg}
\end{equation}

Así pues, realizando una operación sencilla con tenemos:

\begin{equation}
\frac{100}{12} \ \textrm{km / L} \times  2.3521 \ \textrm{mpg} = 19.60 \textrm{mpg}
\end{equation}

Hay que tener algo de cuidado, ya que tenemos un modelo de la forma log-log. Así pues, además de introducir en la variable $X$ que tenemos $\log(19.60)$, obtendremos valores para `log(Price)`. Es por eso que aplicamos la función inversa a la transformación implementada: en este caso la exponencial.


```{r conf_logprice_logmpg}
# Valor de estimación puntual
mpg_value <- 19.60
log_mpg_value <- log(mpg_value)

# Predicción por nuestro modelo
prediction_original_scale <- exp(predict(logreg, newdata = data.frame(log_MPG.city = log_mpg_value), interval = 'confidence'))

cat("Para un MPG.city de", mpg_value, "mpg, la estimación puntual de Price (en miles de dólares) es",
    round(prediction_original_scale[1], 3), "con un intervalo de confianza:", 
    "(", round(prediction_original_scale[2], 3), ",", round(prediction_original_scale[3], 3), ").")
```

```{r pred_logprice_logmpg}
# Valor de estimación puntual
mpg_value <- 19.60
log_mpg_value <- log(mpg_value)

# Predicción por nuestro modelo
prediction_original_scale <- exp(predict(logreg, newdata = data.frame(log_MPG.city = log_mpg_value), interval = 'prediction'))

cat("Para un MPG.city de", mpg_value, "mpg, la estimación puntual de Price (en miles de dólares) es",
    round(prediction_original_scale[1], 3), "con un intervalo de predicción:", 
    "(", round(prediction_original_scale[2], 3), ",", round(prediction_original_scale[3], 3), ").")
```
Puede observarse fácilmente cómo el intervalo de predicción es mucho más ancho que el de confianza. Esto se debe a que en el intervalo de predicción se considera tanto la incertidumbre en la estimación del parámetro de la población como la variabilidad de los puntos de datos individuales alrededor de la línea de regresión. $\\$

Para responder a la pregunta que se nos hace conviene utilizar el que ofrece la predicción. El precio mínimo esperado será de `r round(prediction_original_scale[2], 3)` miles de dólares.

# Ejercicio 3.

1. Considerando un tope de 10 variables, encuentra el número óptimo de variables a incluir en un modelo predictivo de Price, según los criterios R2, BIC y AIC.
- ¿Qué variables incluye el modelo obtenido? (seleccionar el criterio que más te guste). Interpreta los coeficientes obtenidos, ¿consideras que tienen sentido?.

2. Selecciona el mejor modelo con el método stepwise.

3. Selecciona el mejor modelo con el método stepwise considerando la variable Passengers como
factor. Contesta a las siguientes preguntas:

- ¿Qué % de la varianza de Price explica el modelo?
- ¿Podrías depurar el modelo?
- ¿Cuál es el efecto de la variable Origin sobre Price?

4. ¿Qué modelo de los apartados anteriores es mejor? Con el que te quedes, realiza el diagnóstico de tu modelo, sin emprender ninguna acción, e indica los problemas que presenta.

5. Emprende ahora las acciones que te parezcan oportunas e indica los problemas que has conseguido solucionar o mejorar un poco.

6. Obtén la predicción del precio para un coche en la mediana de los predictores en el modelo escogido. Notar que las variables categóricas se tratan de diferente manera, no hay mediana.

## Apartado 1.

Considerando un tope de 10 variables, encuentra el número óptimo de variables a incluir en un modelo predictivo de Price, según los criterios R2, BIC y AIC.
- ¿Qué variables incluye el modelo obtenido? (seleccionar el criterio que más te guste). Interpreta los coeficientes obtenidos, ¿consideras que tienen sentido?.

**Resolución**.

```{r carga_cars_2}
cars <- read.csv2("./data/cars.csv", stringsAsFactors = TRUE)
```

La función `regsubsets()` del paquete `leaps` nos permitirá realizar regresiones por subconjuntos, lo que implica la posibilidad de poder ajustar modelos lineales con todas las combinaciones posibles de un conjunto dado de variables predictoras. Podremos con esta además identificar los valores que toman los modelos según los distintos criterios propuestos ($R^2$, BIC, AIC (llamado aquí CP)). $\\$

A continuación se muestra lo que devuelve esta función al aplicarse a nuestros datos. Obtendremos primeramente una especie de tabla en el que se muestran los mejores modelos (`nbest = 1`) según el número de variables inmiscuidas (de 1 hasta `nvmax`), junto con las variables que este emplea. Se ha podido comprobar cómo se obtienen mejores resultados si no se tienen en cuenta en los cálculos posibles dependencias cuadráticas en las variables predictoras.

```{r ajuste.todo, fig.cap = "\\label{fig:ajuste.todo}Valores obtenidos para distintos criterios en el mejor modelo según el número de variables", fig.pos = "H", fig.align = "center"}

# Implementar variables predictoras cuadráticas
numeric_columns <- colnames(cars)[sapply(cars, is.numeric) & colnames(cars) != "Price"]
variables_cuadraticas <- paste0("I(", numeric_columns, "^2)")
formula <- as.formula(paste("Price ~", paste(numeric_columns, collapse = "+"), "+", paste(variables_cuadraticas, collapse = "+")))

nvmax <- 10

#ajuste.todo <- regsubsets(formula, data = cars, nvmax = nvmax, nbest = 1) # si queremos incluir variables cuadráticas


ajuste.todo <- regsubsets(Price ~ ., data = cars, nvmax = nvmax, nbest = 1) # si solo se quieren las lineales...
resumen <- summary(ajuste.todo)
resumen$outmat

resultado <- cbind(resumen$rsq, resumen$adjr2, resumen$cp, resumen$bic)
colnames(resultado) <- c('Rsq', 'RsqAdj', 'Cp', 'BIC')
resultado

par(mfrow = c(1,3))

plot(1:nvmax, resumen$adjr2, xlab = "# Variables", main = "Coef. Det. Ajustado",
     type="b")
abline(v = which.max(resumen$adjr2), col = 2)

plot(1:nvmax, resumen$cp, xlab = "# Variables", main = "Cp de Mallows",
     type='b')
abline(v = which.min(resumen$cp), col = 2)

plot(1:nvmax, resumen$bic, xlab = "# Variables", main = "BIC",
     type = "b")
abline(v = which.min(resumen$bic), col = 2)

par(mfrow=c(1,1))
```

A la vista de lo obtenido hemos visto adecuado seleccionar el mejor modelo en función de los criterios `CP` o `BIC`, ya que nos permitan reducir algo la complejidad (tendríamos 6 variables en lugar de 10) respecto a si lo hiciéramos con el de `RsqAdj`.

```{r seleccion_modelo_regsubsets_poly}
# ESTE CHUNK DE AQUÍ ES SOLO PARA CUANDO SE INCLUYEN
mejor_modelo_bic <- resumen$which[which.min(resumen$bic), ]


variables_incluidas <- names(which(mejor_modelo_bic))
variables_incluidas <- variables_incluidas[variables_incluidas != "(Intercept)"] 
variables_incluidas_arreglado <- gsub("^Type[^_]*", "Type", variables_incluidas)
variables_incluidas_arreglado <- gsub("^Origin[^_]*", "Origin", variables_incluidas_arreglado)

# Crear la fórmula con términos cuadráticos
formula <- reformulate(variables_incluidas_arreglado, response = "Price")

# Modelo de regresión lineal múltiple con términos cuadráticos
modelo_final <- lm(formula, data = cars)


summary(modelo_final)
```

Las variables que incluye el mejor modelo seleccionado son: {`r variables_incluidas_arreglado`} (resultando significativos sus coeficientes para el modelo, como bien se muestra arriba). Nótese que las variables `Type` y `Origin` han sido consideradas de tipo factor, por lo que, a pesar de que en el mejor modelo obtenido con BIC se seleccionaran solo ciertas categorías, en el modelo seleccionado final debe estar la columna pertinente completa. $\\$

Los resultados obtenidos resultan lógicos si se piensa en el contexto.

- Un incremento en potencia (`Horsepower`) conllevaría un mayor `Precio` en líneas generales.
- Un incremento en la distancia entre centros de ruedas delanteras y traseras (`Wheelbase`) suele indicar que el coche sea algo más grande, lo que puede llevar a un mayor precio.

En cuanto al resto, resulta muy complicado deducir de manera lógica el por qué el coeficiente es positivo o negativo. Se necesitarían explorar otros factores importantes como pudiera ser la demanda de dicho año (1993) en USA de vehículos. Además, una muestra de `nrow(cars)` vehículos no consideramos que sea del todo suficiente (al menos con la percepción actual de la gran cantidad de vehículos que existen).

## Apartado 2.

Selecciona el mejor modelo con el método stepwise.

**Resolución**.

Otro método de selección de un modelo óptimo es el que plantea `stepwise`. Existen dos enfoques:

- **Forward Selection (selección hacia delante)**. Se parte aquí de un modelo vació y se van agregando una a una las variables predictoras, evaluando el impacto de cada adición en el rendimiento del modelo. En cada caso, se agrega la variable que más mejora la métrica de desempeño que estemos utilizando (como BIC), hasta que no se mejore más o se alcance algún criterio de detención.

- **Backward Elimination (eliminación hacia atrás)**. Se empieza con un modelo que incluye todas las variables predictoras y, en cada paso, elimina la variable menos significativa según algún criterio de evaluación (como BIC) hasta que eliminar más variables empeore el modelo o se alcance un criterio de detención.

Existen alternativas que mezclarían los dos métodos.

**Forward stepwise**.

El recorrido se muestra a continuación.

```{r stepwise_selection_forward}
ajuste_0 <- lm(Price ~ 1 , data = cars) # solo el intercepto
traza_forward <- step(ajuste_0, scope = Price ~ Type + MPG.city + 
                        MPG.highway + EngineSize + Horsepower + RPM + 
                        Rev.per.mile + Fuel.tank.capacity + Passengers 
                      + Length + Wheelbase + Width + Weight + Origin,  
                      direction = 'forward')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `forward` es al que posee las variables predictoras `r colnames(traza_forward$model)`. El valor final obtenido para `AIC` ha sido de `r traza_forward$anova$AIC[length(traza_forward$anova$AIC)]`.

**Backward stepwise**.

El recorrido se muestra a continuación.

```{r stepwise_selection_backward}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_backward <- step(ajuste_todo, direction = 'backward')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `backward` es al que posee las variables predictoras `r colnames(traza_backward$model)`. El valor final obtenido para `AIC` ha sido de `r traza_backward$anova$AIC[length(traza_backward$anova$AIC)]`.

**Método híbrido**.

```{r stepwise_selection_both}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_both <- step(ajuste_todo, direction = 'both')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `both` es al que posee las variables predictoras `r colnames(traza_both$model)`. El valor final obtenido para `AIC` ha sido de `r traza_both$anova$AIC[length(traza_both$anova$AIC)]`.

$\\$

El modelo seleccionado será el que obtenga el valor de `AIC` más bajo. 

```{r seleccion_mejor_stepwise}
# Obtener los valores de AIC en un vector
valores_AIC <- c(
  traza_both$anova$AIC[length(traza_both$anova$AIC)],
  traza_forward$anova$AIC[length(traza_forward$anova$AIC)],
  traza_backward$anova$AIC[length(traza_backward$anova$AIC)]
)

# Obtener el nombre del método con el menor valor de AIC
metodos <- c("both", "forward", "backward")
metodo_menor_valor <- metodos[which.min(valores_AIC)]

# El valor mínimo de AIC
valor_minimo_AIC <- min(valores_AIC)

columnas_modelo <- colnames(get(paste("traza_", metodo_menor_valor, sep = ""))$model)
columnas_modelo <- columnas_modelo[columnas_modelo != "Price"]
```

El método seleccionado es el perteneciente a `r metodo_menor_valor`, donde se obtuvo un `AIC` de `r valor_minimo_AIC` y utilizando como variables predictoras `r columnas_modelo`.

## Apartado 3.

Selecciona el mejor modelo con el método stepwise considerando la variable Passengers como
factor. Contesta a las siguientes preguntas:

- ¿Qué % de la varianza de Price explica el modelo?
- ¿Podrías depurar el modelo?
- ¿Cuál es el efecto de la variable Origin sobre Price?

**Resolución**.

Convertimos a factor de manera manual la variable `Passengers` en primer lugar.

```{r conversion_factor_Passengers}
cars$Passengers <- as.factor(cars$Passengers)
```


Se debe ahora repetir lo hecho en el apartado anterior (ver código .Rmd para mostrar las trazas.)

```{r stepwise_selection_forward_pasfac}
ajuste_0 <- lm(Price ~ 1 , data = cars) # solo el intercepto
traza_forward <- step(ajuste_0, scope = Price ~ Type + Price + MPG.city + 
                        MPG.highway + EngineSize + Horsepower + RPM + 
                        Rev.per.mile + Fuel.tank.capacity + Passengers 
                      + Length + Wheelbase + Width + Weight + Origin,  
                      direction = 'forward', trace = 0)
```

```{r stepwise_selection_backward_pasfac}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_backward <- step(ajuste_todo, direction = 'backward', trace = 0)
```

```{r stepwise_selection_both_pasfac}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_both <- step(ajuste_todo, direction = 'both', trace = 1)
```

```{r seleccion_mejor_stepwise_pasfac}
# Obtener los valores de AIC en un vector
valores_AIC <- c(
  traza_both$anova$AIC[length(traza_both$anova$AIC)],
  traza_forward$anova$AIC[length(traza_forward$anova$AIC)],
  traza_backward$anova$AIC[length(traza_backward$anova$AIC)]
)

# Obtener el nombre del método con el menor valor de AIC
metodos <- c("both", "forward", "backward")
metodo_menor_valor <- metodos[which.min(valores_AIC)]

# El valor mínimo de AIC
valor_minimo_AIC <- min(valores_AIC)

columnas_modelo <- colnames(get(paste("traza_", metodo_menor_valor, sep = ""))$model)
columnas_modelo <- columnas_modelo[columnas_modelo != "Price"]
```

El método seleccionado es el perteneciente a `r metodo_menor_valor`, donde se obtuvo un `AIC` de `r valor_minimo_AIC` y utilizando como variables predictoras `r columnas_modelo`. Ahora que tenemos el mejor modelo podemos responder a las preguntas realizadas.

- ¿Qué % de la varianza de Price explica el modelo?

```{r mostrar_mejor_modelo}
formula <- reformulate(columnas_modelo, response = "Price")
model_call <- get(paste("traza_", metodo_menor_valor, sep = ""))$call

mejor_modelo <- eval(model_call)

sum_reg <- summary(mejor_modelo)
sum_reg
```

El valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`.

- ¿Podrías depurar el modelo?

Podría aplicar el método de stepwise en su versión `forward` partiendo del modelo actual, tratando de buscar si se percibe alguna mejora añadiendo variables transformadas. Además, introduciré una transformación logarítmica para la variable `Price` también.

```{r depurar_modelo}
formula <- reformulate(columnas_modelo, response = "log(Price)") # para meter logaritmo

ajuste_0 <- lm(formula, data = cars) # metiendo el logaritmo mi modelo

numeric_columns <- colnames(cars)[sapply(cars, is.numeric) & colnames(cars) != "Price"]
numeric_columns_in_model <- intersect(numeric_columns, columnas_modelo)

# Términos cuadráticos
variables_cuadraticas <- paste0("I(", numeric_columns_in_model, "^2)")

# Términos cúbicos
variables_cubicas <- paste0("I(", numeric_columns_in_model, "^3)")

# Términos logarítmicos
variables_log <- paste0("I(log(", numeric_columns_in_model, "))")

# Fórmula completa añadiendo todo tipo de variables...
formula <- as.formula(paste("log(Price) ~", paste(columnas_modelo, collapse = "+"), "+",
                            paste(variables_cuadraticas, collapse = "+"), "+",
                            paste(variables_cubicas, collapse = "+"), "+",
                            paste(variables_log, collapse = "+")))


traza_forward <- step(ajuste_0, scope = formula,  
                      direction = 'forward', trace = 1)
```

Puede apreciarse que ninguna supone una mejoría de entrada para nuestro modelo, por lo que se corta en la primera iteración. 


- ¿Cuál es el efecto de la variable Origin sobre Price?

El mejor modelo (tras el proceso de depuración anterior) resulta entonces en:

```{r}
columnas_modelo <- colnames(get(paste("traza_", "forward", sep = ""))$model)
columnas_modelo <- columnas_modelo[columnas_modelo != "Price"]

formula <- reformulate(columnas_modelo, response = "Price")
model_call <- get(paste("traza_", metodo_menor_valor, sep = ""))$call

mejor_modelo <- eval(model_call)

sum_reg <- summary(mejor_modelo)
sum_reg
```

Tal y como se observa, el mejor modelo obtenido sí tiene presente dicha variable. Al ser una variable categórica esta viene descompuesta en $k - 1$ predictores codificados como 0 o 1 (variables *dummy*). En este caso, al tener 2 categorías dispondría de una sola variable dummy: `OriginUSA`. Puede apreciarse que esta posee un coeficiente significativo y que se estima como negativo, indicando que si el coche es de origen estadounidense la variable `Price` debería tender a ser menor (con el resto de variables constantes).

## Apartado 4.

¿Qué modelo de los apartados anteriores es mejor? Con el que te quedes, realiza el diagnóstico de tu modelo, sin emprender ninguna acción, e indica los problemas que presenta.

**Resolución**.

El mejor modelo obtenido es el de la forma: **`r paste(deparse(model_call), collapse = "")`**. $\\$

Haciendo uso del comando `plot` sobre nuestro objeto que contiene el modelo de regresión obtenemos una serie de gráficos diagnósticos que nos proporcionan información sobre la idoneidad de nuestro modelo.

```{r plot_general_diag, fig.cap = "\\label{fig:plot_general_diag}Gráficos de diagnóstico del mejor modelo obtenido.", fig.pos = "H", fig.align = "center", fig.width = 7, fig.height = 5.5}
par(mfrow = c(2, 2))
plot(mejor_modelo)
```
Podemos observar en el gráfico de arriba a la izquierda cómo los residuos se colocan alrededor de la línea horizontal de cero, indicando que hay más o menos linealidad. Lo que no parece tan claro es la homocedasticidad, ya que hacia la parte central y final crece un poco la variabilidad. Algo similar se observa en el gráfico de `Scale-Location` con los residuos estandarizados (y en valor absoluto). $\\$

Por otra parte, en el QQ-plot de los residuos estandarizados obtenemos una normalidad prácticamente perfecta, salvando la cola final en la que se aprecian dos puntos que parecen estar dando algo de problema. $\\$

Finalmente, en el gráfico de abajo a la derecha se nos muestran las observaciones que tienen un alto efecto en los coeficientes de regresión y que, por tanto, pueden influir significativamente en la forma del modelo. 


A continuación se muestran algunas gráficas de residuos parciales, para tratar de evaluar la contribución individual de cada variable independiente en nuestro modelo. Son los residuos al eliminar cada predictor.

```{r}
parciales <- residuals(mejor_modelo, type = "partial")

plot(cars$Horsepower, parciales[, "Horsepower"])
abline(h = 0, col = "gray")

plot(cars$RPM, parciales[, "RPM"])
abline(h = 0, col = "gray")  

plot(cars$Wheelbase, parciales[, "Wheelbase"])
abline(h = 0, col = "gray") 
```

