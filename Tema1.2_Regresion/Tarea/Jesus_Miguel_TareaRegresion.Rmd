---
title: "Tarea -- Regresión lineal"
author: "Jesús Martínez Leal y Miguel Muñoz Blat (Grupo 11)"
date: "`r Sys.Date()`"
output:
  html_document:
    echo: yes
    number_sections: no
    theme: readable
    toc: yes
    fig_caption: yes
  pdf_document:
    toc: yes
    fig_caption: yes
  html_notebook: 
    echo: yes
    number_sections: no
    theme: readable
    toc: yes
    fig_caption: yes
params:
  lang: ES
lang: "`r switch(params$lang, ES = 'es-ES', EN = 'en-US')`"
subtitle: "Tema 1 - Aprendizaje Máquina (I). Master Ciencia de Datos UV"
header-includes:
   - \usepackage{multirow}
   - \usepackage{tikz} # Diagramas Latex
   - \usetikzlibrary{positioning}
   - \usepackage{float}
   - \usepackage[font=small,labelfont=bf,labelsep=period]{caption}
   - \usepackage{subcaption}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# Configuración general de chunks
library(knitr)
options(width = 100)
knitr::opts_chunk$set(echo = F, message = T, error = T, warning = F, comment = NA, dpi = 100, tidy = F, cache.path = '.cache/',  fig.path = './figure/', include = T, fig.cap = "", fig.width = 6, fig.height = 4.3)
```

```{r librerias, message = F, include = T, echo = F}
# Carga de librerías necesarias con pacman
library(pacman) # install.packages("packman") en caso de no tenerse.
pacman::p_load(readr, ggplot2, dplyr, tidyr, MASS, leaps, gridExtra, lmtest, car, GGally, ggcorrplot)
```

# Ejercicio 1.

Considera la variable respuesta `Price` relacionándola con la variable `X` con la que tenga mayor relación lineal.

1. Evalúa el efecto de `X` sobre `Price`.

2. Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad,
R2, contraste del modelo, etc...).

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de confianza al 90 %.

4. Realiza un diagnóstico de los residuos. Si falla algunas de las condiciones, busca una posible
solución.


## Carga inicial de los datos necesarios.

```{r carga_cars}
cars <- read.csv2("./data/cars.csv", stringsAsFactors = TRUE)
```

Lo primero de todo será ver un poco en qué consiste nuestro conjunto de datos y en qué unidades se miden las distintas variables.

```{r info_cars}
head(cars)
```
Tras usar el comando `?Cars93` en la terminal de RStudio, observamos que tenemos numerosas variables con características de 93 coches en venta en Estados Unidos en 1993. Vemos que por ejemplo la variable `Price` viene medida en miles de dólares y que `Horsepower` viene medida en HP (lo que serían 746 W en el Sistema Internacional).

## Apartado 1.

Evalúa el efecto de `X` sobre `Price`.

**Resolución**.

Para evaluar el efecto de una variable `X` de nuestro conjunto sobre `Price` haremos una correlación entre los pares de variables. Se utiliza el método de Pearson para su cálculo.

La expresión de la correlación viene dada por:

\begin{equation}
r_{X, Y} = \frac{s_{x,y}} {s_x s_y},
\end{equation}

siendo $s_{x, y}$ la covarianza entre las variables $X$ e $Y$ y $s_{x}, s_{y}$ las desviaciones estándar de las variables $X$ e $Y$, respectivamente.

```{r cor_price}
cors_Price <- cars %>%
  summarise(across(where(is.numeric), ~ cor(Price, ., method = "pearson")))
cors_Price

# Miro cuál es la variable cuyo valor absoluto en correlación es mayor (excluyendo Price)

max_abs_value <- max(abs(cors_Price[!(names(cors_Price) == "Price")]))
var_max_value <- names(cors_Price)[which(abs(cors_Price) == max_abs_value)]

max_value <- cors_Price[[var_max_value]]
```

Como se puede apreciar, la mayor correlación en valor absoluto más cercana a la unidad viene dada por la variable `var_max_value` con un valor de `r max_value`.

## Apartado 2.

Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, R2, contraste del modelo, etc...).

**Resolución**.

Nuestro modelo de regresión lineal se considera de la forma:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon,
\end{equation}

siendo en este caso $Y$ = $Price$ y $X$ = $Horsepower$.

```{r lm_horsepower_price}
reg <- lm(Price ~ Horsepower, data = cars)
sum_reg <- summary(reg) # Resumen del modelo de regresión lineal
sum_reg
confint(reg, level = 0.90)
```
En los coeficientes podemos apreciar que el valor estimado del `intercept` ($\beta_0$) es `r reg$coefficients[1]` y el de la pendiente ($\beta_1$) es `r reg$coefficients[2]`. A pesar de que el intercepto posee un valor negativo (un precio negativo carecería de sentido) se nos indica que no hay significancia, por lo que no se descarta que fuera nulo. En contra, la pendiente sí posee significancia, indicando que efectivamente existe una relación (lineal) entre `Price` y `Horsepower` y que además es positiva. Esto es lo que cabría esperar: una subida del precio del vehículo conforme aumentan los caballos de potencia. Se muestran también los intervalos de confianza al 90% para intercepto y pendiente. Esto permite reafirmar lo comentado anteriormente: el intercepto no se descarta que sea 0, mientras que la pendiente sí es claramente distinta de 0 y además positiva.$\\$

Por otra parte, el valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`. El valor ajustado del $R^2$ viene dado por la expresión:

\begin{equation}
R^2_{\textrm{ajustado}} = 1 - \frac{n - 1}{n - k - 1} (1 - R^2)
\end{equation}

Este tiene en cuenta la penalización incluida por aumentar el número de predictores. El valor obtenido es ligeramente menor: `r sum_reg$adj.r.squared`. $\\$

Finalmente, el p-valor asociado al estadístico F es también sufucientemente pequeño (< 2.2e-16) como para concluir que al menos un coeficiente del modelo lineal es distinto de cero. En este caso, dado que solo hay un predictor, este test sería equivalente al contraste sobre la pendiente de la recta (por eso se obtiene lo mismo).

## Apartado 3.

Dibuja el diagrama de dispersión, la recta de regresión y las bandas de confianza al 90 %.

**Resolución**.

```{r graph_hp_price, fig.cap = "\\label{fig:graph_hp_price}Gráfico de dispersión y recta de regresión para nuestro modelo", fig.pos = "H", fig.align = "center"}

# Gráfico de dispersión con rango en el eje x de 0 a 300
with(cars, plot(Horsepower, Price, col = 'blue', xlab = 'Horsepower / HP', ylab = 'Price / 1000 $', xlim = c(0, 300), ylim = c(-10, 70)))

# Recta de regresión
abline(reg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_hp <- data.frame(Horsepower = seq(0, 300, length.out = 250))
bandas <- predict(reg, newdata = puntos_hp, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_hp$Horsepower, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_hp$Horsepower, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topleft", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
Observamos en la Figura anterior cómo la mayoría de puntos con los que contamos están agrupados en torno a 100 - 200 de Horsepower medido en HP. El modelo lineal realmente solo tiene relevancia en la zona de nuestros datos. Puede observarse cómo las bandas de confianza se acercan mucho más a la recta de regresión donde teníamos más valores agrupados. Es notable apreciar cómo hay un valor muy alejado de nuestra recta de regresión que seguramente está "tirando de ella" hacia arriba.

## Apartado 4.

Realiza un diagnóstico de los residuos. Si falla algunas de las condiciones, busca una posible solución.

**Resolución**

Realizamos primero un diagrama de dispersión para nuestros residuos.

```{r graph_res, fig.cap = "\\label{fig:graph_res}Gráfico de residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
plot(reg$fitted.values, reg$residuals, col='blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```

En el gráfico de residuos frente a valores predichos podemos ver que la variabilidad no es estrictamente constante. Para precios bajos encontramos una menor variabilidad que la hallada para precios intermedios y altos. Esto nos lleva a pensar que no tenemos una homocedasticidad clara. Para tratar de solucionar esto, podemos probar a realizar transformaciones sobre la variable dependiente (Precios).
Por otra parte, la linealidad sí que parece adecuada: los residuos se distribuyen alrededor del 0.

En cuanto a la normalidad de los residuos encontramos esto:

```{r qqres, fig.cap = "\\label{fig:qq_res}QQ-Plot de los residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
qqnorm(reg$residuals, col='blue', main = 'QQ-Plot de los Residuos')
qqline(reg$residuals)

shap <- shapiro.test(reg$residuals)
shap
```

Vemos cómo la distribución de residuos sigue un comportamiento normal pero difiere en gran medida en las colas. Utilizando el test de normalidad de Shapiro-Wilk obtenemos un p-valor de `r shap$p.value`, por lo que se rechaza la hipótesis nula de que la distribución sea normal para un nivel de significancia de 0.05.

Veremos ahora cómo cambian las cosas en nuestro modelo si aplicamos una transformación logarítmica.

### Transformación logarítmica de la variable `Precio`

Realizamos los mismos pasos anteriores pero modificando `Precio` por `Log(Precio)` como variable dependiente.


```{r lm_horsepower_logprice}
logreg <- lm(log(Price) ~ Horsepower, data = cars)
sum_logreg <- summary(logreg) # Resumen del modelo de regresión lineal
sum_logreg
confint(logreg, level = 0.90)
```

Con el modelo logarítmico apreciamos que el valor de $R^2$ ha subido algo, teniendo `r sum_logreg$r.squared`, lo que indicaría en este caso una ligera mejoría de lo que teníamos anteriormente. Esto se debe principalmente a que el "outlier" que se ha comentado anteriormente ha sido reducido en importancia con esta transformación.

```{r graph_hp_logprice, fig.cap = "\\label{fig:graph_hp_logrice}Gráfico de dispersión y recta de regresión para nuestro modelo tras la transformación logarítmica", fig.pos = "H", fig.align = "center"}

# Gráfico de dispersión con rango en el eje x de 0 a 300
with(cars, plot(Horsepower, log(Price), col = 'blue', xlab = 'Horsepower / HP', ylab = 'Log(Price / 1000 $)', xlim = c(0, 300)))

# Recta de regresión
abline(logreg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_hp <- data.frame(Horsepower = seq(0, 300, length.out = 250))
bandas <- predict(logreg, newdata = puntos_hp, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_hp$Horsepower, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_hp$Horsepower, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topleft", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```

```{r graph_logres, fig.cap = "\\label{fig:graph_logres}Gráfico de residuos en nuestro modelo tras la transformación logarítmica", fig.pos = "H", fig.align = "center"}
plot(logreg$fitted.values, logreg$residuals, col = 'blue', xlab = 'Precios predichos', ylab = 'Residuos', main = "Gráfica de Residuos en modelo logarítmico")
abline(h = 0,lty = 2)
```

Puede observarse cómo con el modelo logarítmico se ha podido disminuir ligeramente la heterocedasticidad, en el sentido de que no están tan agrupados los puntos iniciales entre sí.

```{r bptest_14}
bptest_mod <- bptest(logreg)
bptest_mod
```
Aún así, el p-valor asociado vemos que no es suficientemente alto, por lo que implicaría tener heterocedasticidad según este test.


```{r qqlogres, fig.cap = "\\label{fig:qq_logres}QQ-Plot de los residuos en nuestro modelo con transformación logarítmica", fig.pos = "H", fig.align = "center"}
qqnorm(logreg$residuals, col='blue', main = 'QQ-Plot de los Residuos en transformación logarítmica')
qqline(logreg$residuals)

logshap <- shapiro.test(logreg$residuals)
logshap
```
En este caso obtenemos que nuestra distribución de residuos se adapta más a una distribución normal, viéndose además reflejado esto en el shapiro test donde se obtiene un p-valor de `r logshap$p.value` > 0.05. Así pues, no puede rechazarse la hipótesis nula de normalidad.

# Ejercicio 2.

Considera la variable respuesta Price relacionandola con el predictor MPG.city.

1. Evalúa el efecto de MPG.city sobre Price.

2. Obtén la recta mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad,
R2, contraste del modelo, etc...).

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de predicción al 90 %.

4. Realiza un análisis de los residuos.

5. ¿Te parece adecuado haber realizado regresión lineal o es preferible otro tipo de regresión?. Ajusta
el modelo que te parezca más adecuado.

6. ¿Qué precio mínimo se espera para aquellos coches con un consumo de 12 litros a los 100 km por
ciudad? Calcula e interpreta el intervalo de confianza y el de predicción.

## Apartado 1.

Evalúa el efecto de MPG.city sobre Price.

**Resolución**.

En primer lugar, de ojear en `?Cars93` vemos que `MPG.city` es básicamente `Millas por galón estadounidense de ciudad`. Es una de las métricas usadas para evaluar la economía del combustible de un vehículo. Una vez sabido esto ya tenemos un poco más de contexto acerca de lo que estamos haciendo. $\\$

La correlación entre `Price` y `MPG.city` se muestra a continuación.

```{r cor_mpg_price}
cor_Price_MPG <- cor(cars$Price, cars$MPG.city, method = "pearson")
cor_Price_MPG
```
MPG.city y Price presentan una correlación lineal de `r cor_Price_MPG`. Su valor absoluto está algo alejado de la unidad, por lo que la relación no es del todo lineal. Por otra parte, el signo negativo indica que, en general, un aumento de MPG.city conllevaría una disminución del Precio.

## Apartado 2.

Obtén la recta mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, R2, contraste del modelo, etc...).

**Resolución**.

Nuestro modelo de regresión lineal se considera de la forma:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon,
\end{equation}

siendo en este caso $Y$ = $Price$ y $X$ = $MPG.city$.

```{r lm_MPGcity_Price}
reg <- lm(Price ~ MPG.city, data = cars)
sum_reg <- summary(reg) # Resumen del modelo de regresión lineal
sum_reg
confint(reg, level = 0.90)
```
En los coeficientes podemos apreciar que el valor estimado del `intercept` ($\beta_0$) es `r reg$coefficients[1]` y el de la pendiente ($\beta_1$) es `r reg$coefficients[2]`. El intercepto posee un valor claramente distinto de 0 y además cuenta con significancia. El valor de la pendiente por su parte es negativo y cuenta con significancia. Se muestran también los intervalos de confianza al 90% para intercepto y pendiente. Esto permite reafirmar lo comentado anteriormente: el intercepto no se descarta que sea 0, mientras que la pendiente sí es claramente distinta de 0 y además negativa.$\\$

Por otra parte, el valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`. Este valor es ciertamente bajo y está algo lejos de lo que sería lo usualmente aceptable.

El valor ajustado del $R^2$ viene dado por la expresión:

\begin{equation}
R^2_{\textrm{ajustado}} = 1 - \frac{n - 1}{n - k - 1} (1 - R^2)
\end{equation}

Este tiene en cuenta la penalización incluida por aumentar el número de predictores. El valor obtenido es ligeramente menor: `r sum_reg$adj.r.squared`. $\\$

Finalmente, el p-valor asociado al estadístico F es también sufucientemente pequeño (< 3.31e-10) como para concluir que al menos un coeficiente del modelo lineal es distinto de cero. En este caso, dado que solo hay un predictor, este test sería equivalente al contraste sobre la pendiente de la recta (por eso se obtiene lo mismo).

## Apartado 3.

Dibuja el diagrama de dispersión, la recta de regresión y las bandas de predicción al 90 %.

**Resolución**.

```{r graph_mpg_price, fig.cap = "\\label{fig:graph_mpg_price}Gráfico de dispersión y recta de regresión para nuestro modelo", fig.pos = "H", fig.align = "center"}

with(cars, plot(MPG.city, Price, col = 'blue', xlab = 'MPG.city / MPG', ylab = 'Price / 1000 $'))

# Recta de regresión
abline(reg, col = 'red')

# Generación de puntos para las bandas de confianza
puntos_mpg <- data.frame(MPG.city = seq(0, max(cars$MPG.city), length.out = 250))
bandas <- predict(reg, newdata = puntos_mpg, interval = "confidence")

# Líneas para las bandas de confianza
lines(puntos_mpg$MPG.city, bandas[, "lwr"], col = 'black', lty = 2) # Límite inferior
lines(puntos_mpg$MPG.city, bandas[, "upr"], col = 'black', lty = 2) # Límite superior

# Leyenda de regresión y bandas
legend("topright", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
Se hace muy evidente con la representación que la variable `MPG.city` tiene un aspecto algo cuantizado, ya que no posee demasiados valores distintos. Esto hace que para un mismo valor de esta, haya muchos valores de `Price` diferentes.

## Apartado 4.

Realiza un análisis de los residuos.

**Resolución**.

Realizamos primero un diagrama de dispersión para nuestros residuos.

```{r graph_mpg_res, fig.cap = "\\label{fig:graph_mpg_res}Gráfico de residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
plot(reg$fitted.values, reg$residuals, col='blue', xlab = 'Precios predichos / 1000 $', ylab = 'Residuos / 1000 $', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```
En la gráfica de residuos frente a valores predichos podemos ver que la relación entre ambas no es del todo lineal, tal y como podía verse tanto en la gráfica de dispersión anterior como en el valor absoluto de la correlación de Pearson. Vemos además que la variablidad no es constante por lo que también falla la homocedasticidad. 
En cuanto a la normalidad de los residuos encontramos esto:

```{r qqres_mpg, fig.cap = "\\label{fig:qq_mpg_res}QQ-Plot de los residuos en nuestro modelo", fig.pos = "H", fig.align = "center"}
qqnorm(reg$residuals, col='blue', main = 'QQ-Plot de los Residuos')
qqline(reg$residuals)

shap <- shapiro.test(reg$residuals)
shap
```

Por otro lado, en la gráfica de los cuantiles de los residuos frente a los cuantiles teóricos de una distribución normal podemos ver cómo en los extremos los cuantiles se desvían considerablemente, por lo que tampoco parece que se cumpla la normalidad de los residuos. Esto puede verse también con un contraste de hipótesis que nos brinda el test de normalidad de Shapiro-Wilk, en el que se obtiene un p-valor de `r shap$p.value`, por lo que se rechaza la hipótesis nula de que la distribución sea normal para un nivel de significancia de 0.05. $\\$


En definitiva, no se cumplen las hipótesis de partida del modelo lineal, por lo que no sería lo más apropiado para describir la relación entre ambas variables. En todo caso, podría plantearse la transformación de alguna de las variables con tal de obtener mayor linealidad, homocedasticidad y normalidad. También podría plantearse otro modelo de regresión no lineal como el **modelo potencial**, que parece más adecuado para describir la relación entre estas varaibles.

## Apartado 5.

¿Te parece adecuado haber realizado regresión lineal o es preferible otro tipo de regresión? Ajusta el modelo que te parezca más adecuado.

**Resolución**.

Se probarán distintos modelos para ver con cuál se obtienen resultados más óptimos.

### Paso a un modelo potencial.

El modelo potencial que proponemos es de la forma: 

\begin{equation}
Y = \beta_0 X^{\beta_{1}}
\end{equation}

Este puede expresarse como un modelo lineal tomando logaritmos:

\begin{equation}
\log(Y) = \log(\beta_0) + \beta_1 \log(X)
\end{equation}

Pasan a realizarse los pasos seguidos anteriormente aplicando esta última transformación.

```{r cor_logmpg_logprice}
cars$log_MPG.city <- log(cars$MPG.city)

cor_logPrice_logMPG <- cor(log(cars$Price), cars$log_MPG.city, method = "pearson")
cor_logPrice_logMPG
```
Vemos ya que la correlación ha aumentado (en valor absoluto) entre estas dos variables, lo que es una buena señal.

```{r lm_logMPGcity_logPrice}
logreg <- lm(log(Price) ~ log_MPG.city, data = cars)
sum_logreg <- summary(logreg) # Resumen del modelo de regresión lineal
sum_logreg
confint(logreg, level = 0.90)
```
Observamos ya una subida en el valor de $R^2$:`r sum_logreg$r.squared`, indicando que este modelo puede ajustarse de una mejor manera a nuestros datos.

```{r graph_logmpg_logprice, fig.cap = "\\label{fig:graph_logmpg_logprice}Gráfico de dispersión y recta de regresión para nuestro modelo potencial", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}

# Crear el gráfico de dispersión
with(cars, plot(log_MPG.city, log(Price), col = 'blue', xlab = 'log(MPG.city / mpg)', ylab = 'log(Price / 1000 $)'))

# Añadir la línea de regresión
abline(coef = coefficients(logreg), col = 'red')

# Generar puntos para el eje x
puntos_log_mpg.city <- data.frame(log_MPG.city = seq(min(cars$log_MPG.city), max(cars$log_MPG.city), length.out = 200))

# Predecir valores y bandas de confianza
bandas <- predict(logreg, newdata = puntos_log_mpg.city, interval = "confidence")

# Graficar las bandas de confianza
lines(puntos_log_mpg.city$log_MPG.city, bandas[, 2], col = 'black') # Límite inferior
lines(puntos_log_mpg.city$log_MPG.city, bandas[, 3], col = 'black') # Límite superior


# Leyenda de regresión y bandas
legend("topright", legend = c("Recta de Regresión", "Bandas de Confianza al 90%"),
       col = c("red", "black"), lty = c(1, 2))
```
```{r graph_logmpg_res, fig.cap = "\\label{fig:graph_mpg_res}Gráfico de residuos en nuestro nuevo modelo potencial", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
plot(logreg$fitted.values, logreg$residuals, col='blue', xlab = 'Precios predichos', ylab = 'Residuos', main = "Gráfica de Residuos")
abline(h = 0, lty = 2)
```
Vemos cómo el problema de la heterocedasticidad se ha ido en gran medida, al igual que el de la linealidad. $\\$

En cuanto a la normalidad de los residuos podemos aplicar el contraste que nos ofrece el shapiro test nuevamente, obteniendo:

```{r qqlogresmpg, fig.cap = "\\label{fig:qq_logresmpg}QQ-Plot de los residuos en nuestro modelo con transformación potencial", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
qqnorm(logreg$residuals, col='blue', main = 'QQ-Plot de los Residuos con la transformación potencial.')
qqline(logreg$residuals)

logshap <- shapiro.test(logreg$residuals)
logshap
```
Ahora sí vemos que los residuos parecen mucho más normales, obteniendo un p-valor de `r logshap$p.value`: no se puede rechazar la hipótesis nula de normalidad.

## Apartado 6.

¿Qué precio mínimo se espera para aquellos coches con un consumo de 12 litros a los 100 km por ciudad? Calcula e interpreta el intervalo de confianza y el de predicción.

**Resolución**

Para hacer este cálculo debemos primero conocer la conversión a las unidades que estamos tratando con nuestros datos. Una búsqueda en [Convertidor](https://www.advancedconverter.com/es/otros-convertidores/consumo-de-combustible/millas-por-galon-us-a-kilómetros-por-litro) nos muestra que:

\begin{equation}
1 \ \textrm{km / L} = 2.3521 \ \textrm{mpg}
\end{equation}

Así pues, realizando una operación sencilla con tenemos:

\begin{equation}
\frac{100}{12} \ \textrm{km / L} \times  2.3521 \ \textrm{mpg} = 19.60 \textrm{mpg}
\end{equation}

Hay que tener algo de cuidado, ya que tenemos un modelo de la forma log-log. Así pues, además de introducir en la variable $X$ que tenemos $\log(19.60)$, obtendremos valores para `log(Price)`. Es por eso que aplicamos la función inversa a la transformación implementada: en este caso la exponencial.


```{r conf_logprice_logmpg}
# Valor de estimación puntual
mpg_value <- 19.60
log_mpg_value <- log(mpg_value)

# Predicción por nuestro modelo
prediction_original_scale <- exp(predict(logreg, newdata = data.frame(log_MPG.city = log_mpg_value), interval = 'confidence'))

cat("Para un MPG.city de", mpg_value, "mpg, la estimación puntual de Price (en miles de dólares) es \n",
    round(prediction_original_scale[1], 3), "con un intervalo de confianza:", 
    "(", round(prediction_original_scale[2], 3), ",", round(prediction_original_scale[3], 3), ").")
```

```{r pred_logprice_logmpg}
# Valor de estimación puntual
mpg_value <- 19.60
log_mpg_value <- log(mpg_value)

# Predicción por nuestro modelo
prediction_original_scale <- exp(predict(logreg, newdata = data.frame(log_MPG.city = log_mpg_value), interval = 'prediction'))

cat("Para un MPG.city de", mpg_value, "mpg, la estimación puntual de Price (en miles de dólares) es \n",
    round(prediction_original_scale[1], 3), "con un intervalo de predicción:", 
    "(", round(prediction_original_scale[2], 3), ",", round(prediction_original_scale[3], 3), ").")
```
Puede observarse fácilmente cómo el intervalo de predicción es mucho más ancho que el de confianza. Esto se debe a que en el intervalo de predicción se considera tanto la incertidumbre en la estimación del parámetro de la población como la variabilidad de los puntos de datos individuales alrededor de la línea de regresión. $\\$

Para responder a la pregunta que se nos hace conviene utilizar el que ofrece la predicción. El precio mínimo esperado será de `r round(prediction_original_scale[2], 3)` miles de dólares.

# Ejercicio 3.

1. Considerando un tope de 10 variables, encuentra el número óptimo de variables a incluir en un modelo predictivo de Price, según los criterios R2, BIC y AIC.
- ¿Qué variables incluye el modelo obtenido? (seleccionar el criterio que más te guste). Interpreta los coeficientes obtenidos, ¿consideras que tienen sentido?.

2. Selecciona el mejor modelo con el método stepwise.

3. Selecciona el mejor modelo con el método stepwise considerando la variable Passengers como
factor. Contesta a las siguientes preguntas:

- ¿Qué % de la varianza de Price explica el modelo?
- ¿Podrías depurar el modelo?
- ¿Cuál es el efecto de la variable Origin sobre Price?

4. ¿Qué modelo de los apartados anteriores es mejor? Con el que te quedes, realiza el diagnóstico de tu modelo, sin emprender ninguna acción, e indica los problemas que presenta.

5. Emprende ahora las acciones que te parezcan oportunas e indica los problemas que has conseguido solucionar o mejorar un poco.

6. Obtén la predicción del precio para un coche en la mediana de los predictores en el modelo escogido. Notar que las variables categóricas se tratan de diferente manera, no hay mediana.

## Apartado 1.

Considerando un tope de 10 variables, encuentra el número óptimo de variables a incluir en un modelo predictivo de Price, según los criterios R2, BIC y AIC.
- ¿Qué variables incluye el modelo obtenido? (seleccionar el criterio que más te guste). Interpreta los coeficientes obtenidos, ¿consideras que tienen sentido?.

**Resolución**.

```{r carga_cars_2}
cars <- read.csv2("./data/cars.csv", stringsAsFactors = TRUE)
```

La función `regsubsets()` del paquete `leaps` nos permitirá realizar regresiones por subconjuntos, lo que implica la posibilidad de poder ajustar modelos lineales con todas las combinaciones posibles de un conjunto dado de variables predictoras. Podremos con esta además identificar los valores que toman los modelos según los distintos criterios propuestos ($R^2$, BIC, AIC (llamado aquí CP)). $\\$

A continuación se muestra lo que devuelve esta función al aplicarse a nuestros datos. Obtendremos primeramente una especie de tabla en el que se muestran los mejores modelos (`nbest = 1`) según el número de variables inmiscuidas (de 1 hasta `nvmax`), junto con las variables que este emplea. Se ha podido comprobar cómo se obtienen mejores resultados si no se tienen en cuenta en los cálculos posibles dependencias cuadráticas en las variables predictoras.

```{r ajuste.todo, fig.cap = "\\label{fig:ajuste.todo}Valores obtenidos para distintos criterios en el mejor modelo según el número de variables", fig.pos = "H", fig.align = "center"}

# Implementar variables predictoras cuadráticas
numeric_columns <- colnames(cars)[sapply(cars, is.numeric) & colnames(cars) != "Price"]
variables_cuadraticas <- paste0("I(", numeric_columns, "^2)")
formula <- as.formula(paste("Price ~", paste(numeric_columns, collapse = "+"), "+", paste(variables_cuadraticas, collapse = "+")))

nvmax <- 10

#ajuste.todo <- regsubsets(formula, data = cars, nvmax = nvmax, nbest = 1) # si queremos incluir variables cuadráticas


ajuste.todo <- regsubsets(Price ~ ., data = cars, nvmax = nvmax, nbest = 1) # si solo se quieren las lineales...
resumen <- summary(ajuste.todo)
resumen$outmat

resultado <- cbind(resumen$rsq, resumen$adjr2, resumen$cp, resumen$bic)
colnames(resultado) <- c('Rsq', 'RsqAdj', 'Cp', 'BIC')
resultado

par(mfrow = c(1,3))

plot(1:nvmax, resumen$adjr2, xlab = "# Variables", main = "Coef. Det. Ajustado",
     type="b")
abline(v = which.max(resumen$adjr2), col = 2)

plot(1:nvmax, resumen$cp, xlab = "# Variables", main = "Cp de Mallows",
     type='b')
abline(v = which.min(resumen$cp), col = 2)

plot(1:nvmax, resumen$bic, xlab = "# Variables", main = "BIC",
     type = "b")
abline(v = which.min(resumen$bic), col = 2)

par(mfrow=c(1,1))
```

A la vista de lo obtenido hemos visto adecuado seleccionar el mejor modelo en función de los criterios `CP` o `BIC`, ya que nos permitan reducir algo la complejidad (tendríamos 6 variables en lugar de 10) respecto a si lo hiciéramos con el de `RsqAdj`.

```{r seleccion_modelo_regsubsets_poly}
# ESTE CHUNK DE AQUÍ ES SOLO PARA CUANDO SE INCLUYEN
mejor_modelo_bic <- resumen$which[which.min(resumen$cp), ]


variables_incluidas <- names(which(mejor_modelo_bic))
variables_incluidas <- variables_incluidas[variables_incluidas != "(Intercept)"] 
variables_incluidas_arreglado <- gsub("^Type[^_]*", "Type", variables_incluidas)
variables_incluidas_arreglado <- gsub("^Origin[^_]*", "Origin", variables_incluidas_arreglado)

# Crear la fórmula con términos cuadráticos
formula <- reformulate(variables_incluidas_arreglado, response = "Price")

# Modelo de regresión lineal múltiple con términos cuadráticos
modelo_final <- lm(formula, data = cars)


summary(modelo_final)
```

Las variables que incluye el mejor modelo seleccionado son: {`r variables_incluidas_arreglado`} (resultando significativos sus coeficientes para el modelo, como bien se muestra arriba). Nótese que las variables `Type` y `Origin` han sido consideradas de tipo factor, por lo que, a pesar de que en el mejor modelo obtenido con BIC se seleccionaran solo ciertas categorías, en el modelo seleccionado final debe estar la columna pertinente completa. $\\$

Los resultados obtenidos resultan lógicos si se piensa en el contexto.

- Un incremento en potencia (`Horsepower`) conllevaría un mayor `Precio` en líneas generales.
- Un incremento en la distancia entre centros de ruedas delanteras y traseras (`Wheelbase`) suele indicar que el coche sea algo más grande, lo que puede llevar a un mayor precio.

En cuanto al resto, resulta muy complicado deducir de manera lógica el por qué el coeficiente es positivo o negativo. Se necesitarían explorar otros factores importantes como pudiera ser la demanda de dicho año (1993) en USA de vehículos. Además, una muestra de `nrow(cars)` vehículos no consideramos que sea del todo suficiente (al menos con la percepción actual de la gran cantidad de vehículos que existen).

Una cosa que sí llama la atención es que el precio disminuye con la variable `RPM`, ya que mayores Revoluciones Por Minuto proporcionan mayor potencia y uno esperaría un mayor precio. Por ello, podemos estudiar si RPM es realmente significativa realizando un test F parcial, cuya hipótesis nula es que el coeficiente RPM es cero.

```{r anova_RPM}
prueba <- update(modelo_final,~.-RPM)
anova(prueba, modelo_final, test = "F")
```

En este caso podemos rechazar la hipótesis nula para un nivel de significancia de 0.05 por lo que podemos mantener RPM como predictor.

## Apartado 2.

Selecciona el mejor modelo con el método stepwise.

**Resolución**.

Otro método de selección de un modelo óptimo es el que plantea `stepwise`. Existen dos enfoques:

- **Forward Selection (selección hacia delante)**. Se parte aquí de un modelo vació y se van agregando una a una las variables predictoras, evaluando el impacto de cada adición en el rendimiento del modelo. En cada caso, se agrega la variable que más mejora la métrica de desempeño que estemos utilizando (como BIC), hasta que no se mejore más o se alcance algún criterio de detención.

- **Backward Elimination (eliminación hacia atrás)**. Se empieza con un modelo que incluye todas las variables predictoras y, en cada paso, elimina la variable menos significativa según algún criterio de evaluación (como BIC) hasta que eliminar más variables empeore el modelo o se alcance un criterio de detención.

Existen alternativas que mezclarían los dos métodos.

**Forward stepwise**.

El recorrido se muestra a continuación (Ver código, en el PDF ocupa mucho).

```{r stepwise_selection_forward, include = F}
ajuste_0 <- lm(Price ~ 1 , data = cars) # solo el intercepto
traza_forward <- step(ajuste_0, scope = Price ~ Type + MPG.city + 
                        MPG.highway + EngineSize + Horsepower + RPM + 
                        Rev.per.mile + Fuel.tank.capacity + Passengers 
                      + Length + Wheelbase + Width + Weight + Origin,  
                      direction = 'forward')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `forward` es al que posee las variables predictoras `r colnames(traza_forward$model)`. El valor final obtenido para `AIC` ha sido de `r traza_forward$anova$AIC[length(traza_forward$anova$AIC)]`.

**Backward stepwise**.

El recorrido se muestra a continuación (Ver código, en el PDF ocupa mucho).

```{r stepwise_selection_backward, include = F}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_backward <- step(ajuste_todo, direction = 'backward')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `backward` es al que posee las variables predictoras `r colnames(traza_backward$model)`. El valor final obtenido para `AIC` ha sido de `r traza_backward$anova$AIC[length(traza_backward$anova$AIC)]`.

**Método híbrido**.

El recorrido se muestra a continuación (Ver código, en el PDF ocupa mucho).

```{r stepwise_selection_both, include = F}
ajuste_todo <- lm(Price ~ ., data = cars)
traza_both <- step(ajuste_todo, direction = 'both')
```

Vemos que el mejor modelo al que acaba llegando el proceso iterativo en modo `both` es al que posee las variables predictoras `r colnames(traza_both$model)`. El valor final obtenido para `AIC` ha sido de `r traza_both$anova$AIC[length(traza_both$anova$AIC)]`.

$\\$

El modelo seleccionado será el que obtenga el valor de `AIC` más bajo. 

```{r seleccion_mejor_stepwise}
# Obtener los valores de AIC en un vector
valores_AIC <- c(
  traza_both$anova$AIC[length(traza_both$anova$AIC)],
  traza_forward$anova$AIC[length(traza_forward$anova$AIC)],
  traza_backward$anova$AIC[length(traza_backward$anova$AIC)]
)

# Obtener el nombre del método con el menor valor de AIC
metodos <- c("both", "forward", "backward")
metodo_menor_valor <- metodos[which.min(valores_AIC)]

# El valor mínimo de AIC
valor_minimo_AIC <- min(valores_AIC)

columnas_modelo <- colnames(get(paste("traza_", metodo_menor_valor, sep = ""))$model)
columnas_modelo <- columnas_modelo[columnas_modelo != "Price"]
```

El método seleccionado es el perteneciente a `r metodo_menor_valor`, donde se obtuvo un `AIC` de `r valor_minimo_AIC` y utilizando como variables predictoras `r columnas_modelo`.

## Apartado 3.

Selecciona el mejor modelo con el método stepwise considerando la variable Passengers como
factor. Contesta a las siguientes preguntas:

- ¿Qué % de la varianza de Price explica el modelo?
- ¿Podrías depurar el modelo?
- ¿Cuál es el efecto de la variable Origin sobre Price?

**Resolución**.

Convertimos a factor de manera manual la variable `Passengers` en primer lugar. Hacemos una copia de nuestro dataset para no modificarlo: `cars2`.

```{r conversion_factor_Passengers}
cars2 <- cars
cars2$Passengers <- as.factor(cars2$Passengers)
```


Se debe ahora repetir lo hecho en el apartado anterior (ver código .Rmd para mostrar las trazas.)

```{r stepwise_selection_forward_pasfac, include = F}
ajuste_0 <- lm(Price ~ 1 , data = cars2) # solo el intercepto
traza_forward <- step(ajuste_0, scope = Price ~ Type + Price + MPG.city + 
                        MPG.highway + EngineSize + Horsepower + RPM + 
                        Rev.per.mile + Fuel.tank.capacity + Passengers 
                      + Length + Wheelbase + Width + Weight + Origin,  
                      direction = 'forward', trace = 1)
```

```{r stepwise_selection_backward_pasfac, include = F}
ajuste_todo <- lm(Price ~ ., data = cars2)
traza_backward <- step(ajuste_todo, direction = 'backward', trace = 1)
```

```{r stepwise_selection_both_pasfac, include = F}
ajuste_todo <- lm(Price ~ ., data = cars2)
traza_both <- step(ajuste_todo, direction = 'both', trace = 1)
```

```{r seleccion_mejor_stepwise_pasfac}
# Obtener los valores de AIC en un vector
valores_AIC <- c(
  traza_both$anova$AIC[length(traza_both$anova$AIC)],
  traza_forward$anova$AIC[length(traza_forward$anova$AIC)],
  traza_backward$anova$AIC[length(traza_backward$anova$AIC)]
)

# Obtener el nombre del método con el menor valor de AIC
metodos <- c("both", "forward", "backward")
metodo_menor_valor <- metodos[which.min(valores_AIC)]

# El valor mínimo de AIC
valor_minimo_AIC <- min(valores_AIC)

columnas_modelo <- colnames(get(paste("traza_", metodo_menor_valor, sep = ""))$model)
columnas_modelo <- columnas_modelo[columnas_modelo != "Price"]
```

El método seleccionado es el perteneciente a `r metodo_menor_valor`, donde se obtuvo un `AIC` de `r valor_minimo_AIC` y utilizando como variables predictoras `r columnas_modelo`. Ahora que tenemos el mejor modelo podemos responder a las preguntas realizadas.

- ¿Qué % de la varianza de Price explica el modelo?

```{r mostrar_mejor_modelo}
formula <- reformulate(columnas_modelo, response = "Price")
model_call <- get(paste("traza_", metodo_menor_valor, sep = ""))$call

mejor_modelo <- eval(model_call)

sum_reg <- summary(mejor_modelo)
sum_reg
```

El valor obtenido para el coeficiente de determinación $R^2$ ha sido de `r sum_reg$r.squared`. Así pues, `r eval(sum_reg$r.squared) * 100` % sería el porcentaje de variabilidad explicado por el modelo para la variable dependiente `Precio`.

- ¿Podrías depurar el modelo?

Para depurar el modelo podemos en primer lugar comprobar si existe interacción entre el factor Type y el resto de varables numéricas.

```{r depurar_modelo_1}
prueba1 <- lm(Price ~ Horsepower*Type + RPM +  Wheelbase + Width + Origin, data = cars2)
summary(prueba1)

prueba2 <- lm(Price ~ Horsepower + RPM*Type +  Wheelbase + Width + Origin, data = cars2)
summary(prueba2)

prueba3 <- lm(Price ~ Horsepower + RPM +  Wheelbase*Type + Width + Origin, data = cars2)
summary(prueba3)

prueba4 <- lm(Price ~ Horsepower + RPM +  Wheelbase + Width*Type + Origin, data = cars2)
summary(prueba4)
```

Ninguna de las interacciones es significativa y la variabilidad explicada es prácticamente igual por lo que descartamos los modelos anteriores, ya que es preferible tener un modelo con menos parámetros.

También podemos plantearnos agrupar categorías del factor Type como por ejemplo Compact y Small, ya que Small presenta el p-valor más alto y esto podría deberse a que aporta lo mismo que la categoría Compact.

```{r depurar_modelo_2}
cars2$Type <- factor(if_else(cars2$Type == 'Small' | cars2$Type == 'Compact', 'Smallsize', cars2$Type))
cars2$Type <- relevel(cars2$Type, ref = 'Midsize')
lm_completo <- lm(Price ~ . , data = cars2)

sel_lm2 <- step(lm_completo, direction = "both", trace = 0)
summary(sel_lm2)
```
Con este cambio hemos podido obtener un resultado algo mejor, por lo que se convertirá en nuestro `mejor_modelo` 

Para depurar más a fondo el modelo, se podría realizar un diagnóstico del mismo y solucionar posibles problemas de linealidad, homocedasticidad, normalidad, outliers... Esto se llevará a cabo en los próximos apartados.


El mejor modelo (tras el proceso de depuración anterior) resulta entonces en:

```{r mostrar_mejor_modelo_depurado}
formula <- reformulate(columnas_modelo, response = "Price")
model_call <- get(paste("traza_", metodo_menor_valor, sep = ""))$call

mejor_modelo <- eval(model_call)

sum_reg <- summary(mejor_modelo)
sum_reg
```
- ¿Cuál es el efecto de la vairable `Origin` sobre `Price`?

Tal y como se observa, el mejor modelo obtenido sí tiene presente dicha variable. Al ser una variable categórica esta viene descompuesta en $k - 1$ predictores codificados como 0 o 1 (variables *dummy*). En este caso, al tener 2 categorías dispondría de una sola variable dummy: `OriginUSA`. Puede apreciarse que esta posee un coeficiente significativo y que se estima como negativo, indicando que si el coche es de origen estadounidense la variable `Price` debería tender a ser menor (con el resto de variables constantes). Esto puede llegar a tener algo de lógica, ya que coches importados suelen ser más caros por motivos de impuestos, costes de transporte, etc. 

## Apartado 4.

¿Qué modelo de los apartados anteriores es mejor? Con el que te quedes, realiza el diagnóstico de tu modelo, sin emprender ninguna acción, e indica los problemas que presenta.

**Resolución**.

El mejor modelo obtenido es el de la forma: **`r paste(deparse(model_call), collapse = "")`**. $\\$

Haciendo uso del comando `plot` sobre nuestro objeto que contiene el modelo de regresión obtenemos una serie de gráficos diagnósticos que nos proporcionan información sobre la idoneidad de nuestro modelo.

```{r plot_general_diag, fig.cap = "\\label{fig:plot_general_diag}Gráficos de diagnóstico del mejor modelo obtenido.", fig.pos = "H", fig.align = "center", fig.width = 7, fig.height = 5.5}
par(mfrow = c(2, 2))
plot(mejor_modelo)
```
Podemos observar en el gráfico de arriba a la izquierda cómo los residuos se colocan alrededor de la línea horizontal de cero, indicando que hay más o menos linealidad. Lo que no parece tan claro es la homocedasticidad, ya que hacia la parte central y final crece un poco la variabilidad. Algo similar se observa en el gráfico de `Scale-Location` con los residuos estandarizados (y en valor absoluto). Una gráfica de un estilo similar con los residuos studentizados se muestra en mayor tamaño más abajo.$\\$

Por otra parte, en el QQ-plot de los residuos estandarizados obtenemos que varios puntos se van considerablemente de la recta que indica la normalidad. Más adelante, se hará el test de Shapiro-Wilk para verificar si pueden considerarse que siguen una distribución normal o no. $\\$

Finalmente, en el gráfico de abajo a la derecha se nos muestran las observaciones que tienen un alto efecto en los coeficientes de regresión y que, por tanto, pueden influir significativamente en la forma del modelo. Más concretamente, se señalan las observaciones atípicas (residuos fuera de [-2, 2]) e influyentes a posteriori (estadístico de Cook > 0.5 y > 1). $\\$

Representamos los residuos studentizados frente a los valores predichos para analizar la linealidad y la homocedasticidad del modelo.

```{r res_stu, fig.cap = "\\label{fig:res_stu}Residuos estudentizados para nuestro modelo.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
res_stu <- rstudent(mejor_modelo)

ggplot(data = cars, aes(x = fitted(mejor_modelo), y = res_stu)) +
geom_point() + geom_smooth(color = "coral",span = 0.4) + geom_hline(yintercept = 0) +
labs(y = "Studentized Residuals",
x = "Fitted values") +
theme_bw()
```
De nuevo, apreciamos cómo no tenemos una distribución de los residuos del todo uniforme alrededor del cero, apreciando en los extremos una subida.

Puede hacerse el test de Breusch-Pagan para homocedasticidad del modelo. La hipótesis nula de este test es que no hay heterocedasticidad en los residuos.

```{r bptest}
bptest_mod <- bptest(mejor_modelo)
bptest_mod
```
Se ha obtenido un p-valor de `r bptest_mod$p.value`, por lo que se podría decir que rechazamos la hipótesis nula: hay heterocedasticidad con un nivel de confianza de 0.05. $\\$

A continuación se muestran algunas gráficas de residuos parciales, para tratar de evaluar la contribución individual de cada variable independiente en nuestro modelo. Son los residuos al eliminar cada predictor.


```{r component_residual, fig.cap = "\\label{fig:component_residual}Gráficos parciales de residuos (componentes + residuos) de nuestro modelo.", fig.pos = "H", fig.align = "center", fig.width = 7, fig.height = 5.5}
crPlots(mejor_modelo, ylab = "Component + Residual", layout = c(2, 3))
```

No se identifican patrones no lineales ni violaciones de la homocedasticidad en estos gráficos. $\\$

Los residuos studentizados se muestran a continuación, junto con el test de shapiro para determinar de forma algo más rigurosa si pueden considerarse normales o no.

```{r normal_res_stu, fig.cap = "\\label{fig:normal_res_stu}Normalidad en los residuos studentizados.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
qqnorm(res_stu)
qqline(res_stu,col="red")
```

```{r shap_test}
shap_test <- shapiro.test(res_stu)
shap_test
```

El p-valor del Shapiro-Wilk test de normalidad es muy pequeño (`r shap_test$p.value`) e indica que la distribución de los residuos no es normal. Esto puede verse en la gráfica de los cuantiles de los residuos frente a los cuantiles teóricos de una distribución normal, ya que en los extremos se alejan bastante de la distribución normal, especialmente el último cuantil de la muestra. Falla por tanto la normalidad del modelo. $\\$

Pasamos ahora sí a analizar los posibles `outliers`. Usualmente un dato será anómalo si el residuo studentizado está fuera de rango (-2, 2) y extremo si está fuera de (-3, 3). Utilizaremos la función `outlierTest()` para encontrarlos.

```{r outliertest}
outliertest <- outlierTest(mejor_modelo)
outliertest

cars2[names(outliertest$p), ]
```
Observamos que la única observación catalogada como outlier ha sido la número 59, la cual ya habíamos observado previamente en la gráfica de residuos studentizados frente a valores predichos de la variable dependiente. Vemos que su valor de residuo studentizado es muy elevado (`r outliertest$rstudent`). $\\$

Podemos averiguar ahora si este outlier es una información influyente. Para ello, en primer lugar representamos la distancia de Cook, que es una medida de la influencia de una observación en la estimación de los coeficientes del modelo, frente a los valores ajustados.

Una observación será influyente si tiene valores para la distancia de Cooks que cumplen:

\begin{equation}
D_i > \frac{4}{n - p - 1},
\end{equation}

siendo $n$ el número de muestras, $p$ el número de coeficientes estimados en el modelo (incluyendo el intercepto).

```{r distcook, fig.cap = "\\label{fig:distcook}Distancias de Cook para los valores ajustados en nuestro modelo..", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
n <- nrow(cars2)
p <- nrow(sum_reg$coefficients) # número de parámetros en el modelo.

Di <- cooks.distance(mejor_modelo)

plot(fitted(mejor_modelo), Di, main = "Distancia de Cook para los valores ajustados", xlab = "Fitted values", ylab = "Cook's distance")
abline(h = 4 / (n - p - 1), col="red", lwd = 1)

boxplot(cooks.distance(mejor_modelo), main = "Boxplot para distancias de Cook", xlab = "DistCook")

sum_cook <- summary(cooks.distance(mejor_modelo))
sum_cook
```

Vemos como hay dos valores con distancia de Cook especialmente grandes, siendo el valor máximo de `r sum_cook["Max."]` mientras que la mediana es `r sum_cook["Median"]`. Si bien el consenso teórico es que una distancia de Cook lo bastante grande si es mayor que uno, podemos considerar que estos valores (en especial el máximo) son suficientemente grandes como para ser analizados individualmente.

A continuación, usamos la función de `influencePlot`. Esta función representa los residuos studentizados frente a los `hat-values` (valores de `leverage`), que son medidas que indican cuánto una observación específica influye en la estimación de sus propios valores ajustados y con un código tamaño y color dado por la distancia de Cook.

```{r influencePlot, fig.cap = "\\label{fig:influencePlot}Gráfico de influencia para nuestro modelo (Distancia de cook y leverage).", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
infPlot <- influencePlot(mejor_modelo)
infPlot
```

Vemos como el outlier que habíamos localizado, correspondiente a la observación 59, es una observación influyente con la mayor distancia de Cook. Un resumen de las medidas de influencias se muestra a continuación.

```{r influence_measures}
summary(influence.measures(mejor_modelo))
```

Las observaciones influyentes dadas por la función influence.measures también incluyen este outlier, que además es la observación que presenta mayor número de valores significativos para los DfBetas (dfb). Las DfBetas muestran las diferencias en los coeficientes estimados al excluir la observación, por lo que este outlier podría estar afectando bastante al modelo. $\\$

Finalmente, vamos a analizar la colinealidad de los predictores. $\\$

La colinealidad es cuando dos o más predictores están muy correlacionados. Se encuentra lo siguiente cuando la hay:

- Es difícil separar el efecto individual de cada predictor.
- Las varianzas de los predictores estarán infladas, afectando al test t y al intervalo de confianza.


Veamos primero gráficos por pares para las variables presentes en nuestro modelo:

```{r ggpairs_modelo, fig.cap = "\\label{fig:pairs}Gráficos por pares para las variables numéricas presentes en nuestro modelo.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
cols_to_plot <- c('Horsepower', 'RPM', 'Width', 'Wheelbase')
ggpairs(cars2[, cols_to_plot], progress = FALSE)
```

Representando por pares las variables numéricas de nuestro modelo vemos que existe una relación lineal entre varias variables, especialmente entre las variables `Wheelbase` y `Width`. Esto era de esperar, ya que la distancia entre ruedas traseras y delanteras intuitivamente puede estar relacionada con la anchura del propio vehículo. Es coherente que las dimensiones de los coches mantengan una determinada proporción por motivos de diseño, reflejándose en esta relación.


```{r cor_modelo, fig.cap = "\\label{fig:cor_modelo}Mapa de correlaciones para las variables numéricas en el modelo.", fig.pos = "H", fig.align = "center", fig.width = 3.5, fig.height = 2.5}
cor(cars2[, cols_to_plot])

correlation_matrix <- cor(cars2[, cols_to_plot])
ggcorrplot(correlation_matrix)
```

Tal y como reflejaban las gráficas, la correlación entre estas variables es algo alta, siendo la mayor correlación entre las variables `Wheelbase` y `Width`, lo cual tiene sentido por lo explicado anteriormente. 

Para ver la multicolinealidad, calculamos el factor de inflación de la varianza (`VIF`). El VIF se calcula para cada variable independiente en nuestro modelo y proporciona una medida de cuánto aumenta la varianza de un coeficiente estimado debido a la multicolinealidad con las otras variables predictoras. La expresión es la siguiente:

\begin{equation}
\textrm{VIF}_j = (1 - R^2_j)^{-1},
\end{equation}
donde $R^2_j$ es el coeficiente de determinación de la regresión $X_j$, como variable respuesta, frente a los demás predictores.

Nos deberíamos preocupar cuando VIF$_j$ > 5.


```{r vif_modelo}
vif(mejor_modelo)
```
Sumado a la alta correlación de la variable `Width` con el resto de variables, tenemos que esta es la que tiene el VIF más alto, por lo que la multicolinealidad es significativa. $\\$

## Apartado 5. 

Emprende ahora las acciones que te parezcan oportunas e indica los problemas que has conseguido solucionar o mejorar un poco.

**Resolución**.


En primer lugar, para mejorar la homocedasticidad del modelo podemos realizar alguna transformación sobre la variable dependiente. En concreto, vamos a buscar una transformación de Box-Cox. Esta es posible de ser realizada puesto que nuestra variable dependiente `Price` es no negativa. Posee la siguiente expresión:

\[
g(y | \lambda) =
\begin{cases}
  \frac{y^{\lambda} - 1}{\lambda} & \text{si } \lambda \neq 0 \\
  \log(y) & \text{si } \lambda = 0
\end{cases}
\]

```{r boxcox, fig.cap = "\\label{fig:boxcox}Gráfica para estimar el valor de lambda necesario en la transformación Box-cox.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
box_cox_data <- boxcox(mejor_modelo, lambda = seq(-2, 2, length = 10))
box_cox_data$x[which.max(box_cox_data$y)]
```

En este caso la lambda que maximiza la verosimilitud sería -0.22. Este es un valor bastante próximo a 0 y en las transformaciones de Box-Cox se toma el logaritmo cuando lambda tiende a cero. Por tanto, por simplicidad podemos tomar directamente una transformación logarítmica, que además tiene una interpretación más directa. Se comparará también con el valor que se obtendría para el $R^2$ ajustado con la transformación exacta. $\\$

Por otro lado, eliminamos la observación 59, que es un outlier y una observación influyente que puede estar afectando demasiado a los coeficietes del modelo.

Finalmente, como hemos visto que `Width` presenta una alta correlación con otras variables como `Length` y `Wheelbase` y una alta multicolinealidad y podría estar aportando información redundante, nos planteamos un modelo sin esta variable. Veremos si mejora el $R^2$ ajustado o por si el contrario no lo hace. 

```{r nuevo_modelo_log_width}
cars3 <- cars[-59,]
modelo_mod_log_width <- lm(log(Price) ~ Type + Horsepower + Width +  RPM + Wheelbase + Origin, data = cars3)
summary(modelo_mod_log_width)
```

```{r nuevo_modelo_box_width}
cars3 <- cars[-59,]
modelo_mod_box_width <- lm((Price^(-0.22) - 1) / -0.22 ~ Type + Horsepower + Width +  RPM + Wheelbase + Origin, data = cars3)
summary(modelo_mod_box_width)
```

No consideramos que merezca la pena complicar tanto el modelo por una ganancia tan escasa en el $R^2$ ajustado, por lo que continuaremos con la transformación logarítmica. Ahora bien, pasamos ahora a ver qué sucede si quitamos la variable predictora `Width`.

```{r nuevo_modelo_log}
cars3 <- cars[-59,]
modelo_mod_log <- lm(log(Price) ~ Type + Horsepower +  RPM + Wheelbase + Origin, data = cars3)
summary(modelo_mod_log)
```
Observamos una caída en el valor de $R^2$ ajustado. Por tanto, preferimos continuar capturando en nuestro modelo dicha variable predictora.

```{r asignacion_modelo_modificado}
modelo_mod <- modelo_mod_log
sum_reg_mod <- summary(modelo_mod)
```

Empezamos mostrando el gráfico general para el diagnóstico con este nuevo modelo:

```{r plot_general_diag_mod, fig.cap = "\\label{fig:plot_general_diag_mod}Gráficos de diagnóstico del modelo modificado.", fig.pos = "H", fig.align = "center", fig.width = 7, fig.height = 5.5}
par(mfrow = c(2, 2))
plot(modelo_mod)
```

Puede observarse a simple vista que la gráfica de residuos frente a valores ajustados ha mejorado.


Veamos las cosas con mayor detalle.

```{r res_modelo_mod, fig.cap = "\\label{fig:res_modelo_mod}Residuos estudentizados para nuestro modelo modificado.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
res_stu <- rstudent(modelo_mod)

ggplot(data = cars3, aes(x = fitted(modelo_mod), y = res_stu)) +
geom_point() + geom_smooth(color = "coral",span=0.4) + geom_hline(yintercept = 0) +
labs(y = "Studentized Residuals",
x = "Fitted values") +
theme_bw()
```

Ahora los residuos studentizados parecen distribuirse de forma más uniforme entorno al cero y la variablidad también es más constante. La ausencia del outlier de la observación 59 también proporciona mejor resultado en la parte derecha del gráfico.

```{r bptest_new}
bptest_mod_new <- bptest(modelo_mod)
bptest_mod_new
```

Ahora el test de homocedasticidad proporciona un p-valor bastante más alto (`r bptest_mod_new$p.value`), lo que confirma la mejora en este aspecto. Ahora no se podría rechazar que haya homocedasticidad.

```{r component_residual_mod, fig.cap = "\\label{fig:component_residual_mod}Gráficos parciales de residuos (componentes + residuos) de nuestro modelo modificado.", fig.pos = "H", fig.align = "center", fig.width = 7, fig.height = 5.5}
crPlots(modelo_mod, ylab = "Component + Residual", layout = c(2, 3))
```

Los residuos parciales siguen sin mostrar grandes problemas de linealidad ni homocedasticidad. El único comportamiento algo extraño se sigue dando por la variable `RPM`.$\\$

Pasamos a comprobar de nuevo la normalidad de los residuos studentizados.

```{r normal_res_stu_mod, fig.cap = "\\label{fig:normal_res_stu_mod}Normalidad en los residuos studentizados.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
qqnorm(res_stu)
qqline(res_stu,col="red")
```


```{r shap_test_new}
shap_test_new <- shapiro.test(res_stu)
shap_test_new
```

El Shapiro-Wilk test ahora no nos permite rechazar la hipótesis nula de que la distribución es normal con un nivel de significancia de 0.05 (el p-valor obtenido ha sido `r shap_test_new$p.value`). Si bien los cuantiles de los residuos studentizados todavía presentan desviaciones en los extremos, estas desviaciones son algo menos pronunciadas. Sobre todo, ya no observamos ese valor claramente más desviado que el resto, que presumiblemente se correspondía al outlier eliminado y empeoraba bastante la normalidad de los residuos. $\\$

Analizamos ahora los outliers del nuevo modelo:

```{r outliertest_new}
outliertest_new <- outlierTest(modelo_mod)
outliertest_new

cars3[names(outliertest_new$p), ]
```
Esta vez no se han encontrado outliers con el test de Bonferroni, como era de esperar. Aún así, la función `outlierTest()` ha retornado el valor que mayor residuo studentizado posee. Vemos que es ligerísimamente superior a 3. Por tanto, considerarlo como un outlier puede ser demasiado arbitrario. Analizamos ahora si este posible outlier puede ser una observación influyente.

```{r distcook_new, fig.cap = "\\label{fig:distcook_new}Distancias de Cook para los valores ajustados en nuestro modelo modificado.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
n <- nrow(cars3)
p <- nrow(sum_reg_mod$coefficients)

Di <- cooks.distance(modelo_mod)

plot(fitted(modelo_mod), Di, main = "Distancia de Cook para los valores ajustados", xlab = "Fitted values", ylab = "Cook's distance")
abline(h = 4 / (n - p - 1), col="red", lwd = 1)

boxplot(cooks.distance(modelo_mod), main = "Boxplot para distancias de Cook", xlab = "DistCook")

sum_cook <- summary(cooks.distance(modelo_mod))
sum_cook
```

En esta ocasión ya no encontramos observaciones con una distancia de Cook tan grande como antes, sino que ahora el valor máximo se queda en `r sum_cook["Max."].

```{r influencePlot_new, fig.cap = "\\label{fig:influencePlot_new}Gráfico de influencia para nuestro modelo modificado(Distancia de cook y leverage).", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
infPlot <- influencePlot(modelo_mod)
infPlot
```
Tenemos ahora que las observaciones no poseen en ningún caso una distancia de Cook que nos pueda preocupar. Además, las que poseen un valor más alto tienen un leverage relativamente bajo.


```{r influence_measures_new}
summary(influence.measures(modelo_mod))
```

La función influence.measures muestra que esta observación tampoco afecta de forma importante a los coeficientes, ya que los DfBetas no son significativos. Por tanto, podemos conservar esta observación, ya que si bien el criterio utilizado la indica como outlier, no tiene una influencia desmesurada en nuestro modelo por lo que no debería ser un problema.

Finalmente, vamos a analizar la colinealidad:

```{r ggpairs_modelo_new, fig.cap = "\\label{fig:pairs_new}Gráficos por pares para las variables numéricas presentes en nuestro modelo.", fig.pos = "H", fig.align = "center", fig.width = 5, fig.height = 4}
cols_to_plot <- c('Horsepower', 'RPM', 'Width', 'Wheelbase')
ggpairs(cars3[, cols_to_plot], progress = FALSE)
```

```{r cor_modelo_new, fig.cap = "\\label{fig:cor_modelo_mod}Mapa de correlaciones para las variables numéricas en el modelo modificado.", fig.pos = "H", fig.align = "center", fig.width = 3.5, fig.height = 2.5}
cor(cars3[, cols_to_plot])

correlation_matrix <- cor(cars3[, cols_to_plot])
ggcorrplot(correlation_matrix)
```

Obviamente las correlaciones entre las variables se mantienen iguales ya que sólo hemos eliminado una observación de nuestro conjunto de datos.

```{r vif_modelo_new}
vif(modelo_mod)
```
Por otro lado, al eliminar la variable `Width` del modelo, encontramos una mejoría apreciable en VIF. La única con un VIF algo superior a 5 sería `Wheelbase`, pero esto no resulta demasiado preocupante. $\\$


En definitiva, con las modificaciones del modelo se han podido solucionar los problemas de homocedasticidad, normalidad de los residuos, además de la colinealidad de las variables del modelo, obteniendo prácticamente la misma varianza explicada.

## Apartado 6.

Obtén la predicción del precio para un coche en la mediana de los predictores en el modelo escogido. *Notar que las variables categóricas se tratan de diferente manera, no hay mediana*.

**Resolución**.


Con el modelo adquirido final (llamado `modelo_mod` en el código), podemos predecir ahora el precio de un coche en la situación que se nos pide. Para el caso de las variables de tipo categórico hemos escogido la moda como representante. Recordemos que nuestro modelo es de la forma:



```{r prediccion_final}
x0 <- data.frame(
  Type = levels(cars3$Type)[which.max(table(cars3$Type))],
  Horsepower = median(cars3$Horsepower),
  RPM = median(cars3$RPM),
  Length = median(cars3$Width),
  Wheelbase = median(cars3$Wheelbase),
  Origin = levels(cars3$Origin)[which.max(table(cars3$Origin))]
)

prediction_original_scale <- exp(predict(modelo_mod, newdata = x0, interval = 'prediction'))

cat("Las características del coche para la predicción vienen dadas por: \n")

print(x0)

cat("La estimación puntual de Price (en miles de dólares) para un coche con esas características es \n",
    round(prediction_original_scale[1], 3), "con un intervalo de predicción:", 
    "(", round(prediction_original_scale[2], 3), ",", round(prediction_original_scale[3], 3), ").")
```

